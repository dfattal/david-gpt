# RAG Product Requirements Document

> **Project**: david-gpt – Multi-persona RAG Platform
> **Scope**: Persona-aware retrieval-augmented generation (RAG) for scalable ingestion and reliable, cited answers.

---

## 1. Goals

- Allow each persona (e.g. david) to answer questions using its own curated knowledge base.
- Keep persona profile and RAG docs in sync in one folder:
  ```
  /personas/<slug>/
  ├ persona.md – natural-language profile
  ├ persona.config.json – generated topics/aliases/router rules
  └ RAG/ – markdown docs for ingestion
  ```
- Simple ingestion pipeline that requires minimal human curation.
- Provide trustworthy citations in final answers.

---

## 2. Folder & File Layout

```
/personas
    persona_template.md
    sample-doc.md
    README.md  
   /<slug>
      persona.md
      persona.config.json        # generated by LLM from persona.md    
      /RAW-DOCS
        2025-odyssey-press.pdf
        leia-runtime-blog.html
        tracking-notes.md
        samsung-links-list.md      # URL list 
      /RAG                       # all ingestible docs
         doc1.md                # auto-generated *.md with frontmatter
         doc2.md
      /QA-QUEUE                   # flagged for manual review
        doc3.md                 # md that need edits
```

All assets that define a persona stay together, ensuring profile ↔ RAG sync.

---

## 3. Document Format for RAG

Each RAG document is a single Markdown file:

```yaml
---
id: unique-stable-slug
title: Document Title
date: YYYY-MM-DD
source_url: https://original/source
type: blog|press|spec|tech_memo|faq|slide|email
personas: [<slug>]
topics: [topic-id-1, topic-id-2]
summary: "One-sentence abstract"
license: public|cc-by|proprietary
---
# Body in Markdown
…
```

Frontmatter fields are minimal and stable; topics must be valid IDs from the persona’s config.

---

## 4. Persona Configuration

### 4.1 persona.md

Free-form natural language describing the persona’s expertise.

### 4.2 persona.config.json

Generated from `persona.md` by an LLM pass:

```json
{
  "slug": "<slug>",
  "display_name": "...",
  "in_scope_examples": [...],
  "out_of_scope_examples": [...],
  "topics": [
    {"id": "switchable-2d3d", "aliases": ["DLB","LC lens","3D cell"]},
    ...
  ],
  "router": {
    "vector_threshold": 0.35,
    "bm25_keywords_min_hits": 2,
    "min_supporting_docs": 2
  }
}
```

This JSON drives:
- query routing (RAG vs. no-RAG)
- topic validation during ingestion.

---

## 5. Ingestion Workflow

### 5.1 Preparing RAW content

RAW could be a PDF, URL, Markdown, slides, or text.
Steps:
1. Extract clean text – e.g. pandoc, trafilatura, pdftotext, OCR if needed.
2. Normalise to Markdown – keep logical headings, short paragraphs.

### 5.2 Prompt to generate ingestible doc

(paste RAW text and persona info to LLM)

> You are preparing a RAG document for persona `<slug>`.
>
> Given the RAW content below, output a single Markdown file:
>
> 1. YAML frontmatter with:
>    - id: stable kebab-case slug
>    - title
>    - date (YYYY-MM-DD if known)
>    - source_url (if known)
>    - type: one of [blog|press|spec|tech_memo|faq|slide|email]
>    - personas: [`<slug>`]
>    - topics: choose from persona.config.json topics (match aliases if needed)
>    - summary: 1–2 sentence abstract
>    - license: public|cc-by|proprietary (omit if unknown)
>
> 2. Clean Markdown body with proper headings.
> 3. Do not invent information; omit any unknown fields.

Review the output, ensure topics are valid IDs, then save in `/personas/<slug>/RAG/`.

---

## 6. Database Schema (conceptual)

- `personas(slug, config_json)`
- `docs(id, title, date, source_url, type, summary, license)`
- `doc_personas(doc_id, persona_slug)`
- `doc_topics(doc_id, topic_id)`
- `chunks(id, doc_id, section_path, text, token_count)`
- `embeddings(chunk_id, vector)`

Optional: trigram index on `chunks.text` for lexical/BM25 search.

---

## 7. Retrieval Strategy

### 7.1 Router: decide if RAG is needed

1. **Heuristic gate**
   Does query hit any topic alias or exceed vector-similarity threshold to any topic embedding?
2. **LLM classifier**

   > Given persona topics and these query examples,
   > classify the question as:
   > - IN_SCOPE_RAG (needs persona docs),
   > - IN_SCOPE_NO_RAG (general knowledge, no citation),
   > - OUT_OF_SCOPE.
   > Output JSON `{classification: "...", requires_citations: true|false}`.

3. **Action**:
   - `IN_SCOPE_RAG` → run retrieval
   - `IN_SCOPE_NO_RAG` → answer directly
   - `OUT_OF_SCOPE` → hand off to general model.

---

### 7.2 RAG retrieval pipeline

Filter by `persona_slug` (and optional matched topics).
1. **Hybrid search**
   - Vector search on chunk embeddings (top-30).
   - BM25 / trigram lexical search on chunk text (top-30).
   - Fuse with Reciprocal Rank Fusion (RRF) → top-20.
2. **Cross-encoder rerank** → top-8–12.
3. **De-duplicate by doc**, prefer 2–3 best chunks per document.

---

### 7.3 Prompt to LLM for answer with citations

> **SYSTEM**:
> You are `<display_name>`. Use ONLY the provided context for persona-specific facts.
> - Every factual statement that depends on the context must include a bracket citation:
>   `[^doc_id:section]`.
> - If the context is insufficient, say so and suggest what doc is missing.
>
> **USER**: {query}
>
> **CONTEXT**:
> ```
> [doc {doc_id} §{section_path}]
> {text}
> ```

Post-process bracket cites into footnotes linking `source_url` + heading anchor.

---

## 8. Citations UX

- Inline bracket `[^doc_id:section]` inside the answer.
- Sources list at the bottom:

  `[^leia-2024-sr-runtime-overview:Tracking] Leia SR Runtime Overview (2024), §Tracking. (link)`

---

## 9. Scaling Guidelines

- **New persona**: add `persona.md` → regenerate `persona.config.json` → add RAG docs.
- **New docs**: drop into `/personas/<slug>/RAG/` and re-run ingestion.
- **Cross-persona docs**: list multiple personas in frontmatter.

---

## 10. Optional Enhancements

- Time-decay ranking for freshness.
- L2 “answer cache” for frequently asked questions.
- Scheduled CI job to check:
  - all docs in RAG folder reference the local persona slug.
  - `persona.config.json` matches `persona.md`.

---

## 11. Out-of-Scope Handling

When router outputs `OUT_OF_SCOPE`:
- Return a polite deferral:
  > “This question is outside `<display_name>`’s expertise; switching to the general assistant.”
- Forward query to a general LLM.
