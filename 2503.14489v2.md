---
title: "S TABLE V IRTUAL C AMERA: Generative View Synthesis with Diffusion Models"
docType: paper
url: "https://arxiv.org/abs/2503.14489v2"
scraped_at: "2025-09-26T19:55:37.963Z"
word_count: 6389
extraction_quality: "high"
authors:
  - name: "Jensen (Jinghao) Zhou"
    affiliation: "Stability AI, University of Oxford"
  - name: "Hang Gao"
    affiliation: "Stability AI, University of California, Berkeley"
  - name: "Vikram Voleti"
    affiliation: "Stability AI"
  - name: "Aaryaman Vasishta"
    affiliation: "Stability AI"
  - name: "Chun-Han Yao"
    affiliation: "Stability AI"
  - name: "Mark Boss"
    affiliation: "Stability AI"
  - name: "Philip Torr"
    affiliation: "University of Oxford"
  - name: "Christian Rupprecht"
    affiliation: "University of Oxford"
  - name: "Varun Jampani"
    affiliation: "Stability AI"
venue: "arXiv"
publicationYear: 2025
abstract: "We present S TABLE V IRTUAL C AMERA (S EVA), a generalist diffusion model that creates novel views of a scene, given any number of input views and target cameras. Existing works struggle to generate either large viewpoint changes or temporally smooth samples, while relying on specific task configurations. Our approach overcomes these limitations through simple model design, optimized training recipe, and flexible sampling strategy that generalize across view synthesis tasks at test time. As a result, our samples maintain high consistency without requiring additional 3D representation-based distillation, thus streamlining view synthesis in the wild. Furthermore, we show that our method can generate high-quality videos lasting up to half a minute with seamless loop closure. Extensive benchmarking demonstrates that S EVA outperforms existing methods across different datasets and settings."
keywords: ["generative view synthesis", "diffusion models", "novel view synthesis", "3d consistency", "video generation"]
technologies: ["lightfield", "autostereoscopic displays", "holography", "computer vision", "diffusion models", "NeRF", "3D Gaussian Splatting"]
---

# S TABLE V IRTUAL C AMERA: Generative View Synthesis with Diffusion Models

## 1. Introduction

Novel view synthesis (NVS) aims to generate realistic, 3Dconsistent images of a scene from arbitrary camera viewpoints given any number of camera-posed input views. Traditional methods, which rely on dense input views, treat NVS as a 3D reconstruction and rendering problem [1–3], but this approach fails with sparse inputs. Generative view synthesis addresses this limitation by leveraging modern deep network priors [4, 5], enabling immersive 3D interactions in uncontrolled environments without the need to capture large image sets per scene. In this work, we focus on generative view synthesis and, unless otherwise specified, refer to it simply as NVS for clarity.

Despite recent progress [6–12], NVS in the wild remains limited due to two key challenges: First, existing methods struggle to generate both large viewpoint changes [9, 10] and temporally smooth samples [6–8, 13] while being constrained by rigid task configurations, such as a fixed number of input and target views [7, 9, 11, 12], reviewed in Tab. 1. Second, their sampling consistency is often insufficient, necessitating additional NeRF distillation to fuse inconsistent results into a coherent representation [7, 8, 13]. These limitations hinder their applicability across diverse NVS tasks, which we address in this work.

We present S TABLE V IRTUAL C AMERA (S EVA), a diffusion-based NVS model that generalizes across a spectrum of view synthesis tasks without requiring NeRF distillation. With a single network, S EVA generates high-quality novel views that strike both large viewpoint changes and temporal smoothness, while supporting any number of input and target views. Our approach simplifies the NVS pipeline without requiring distillation from a 3D representation, thus streamlining it for real-world applications. For the first time, we demonstrate high-quality videos lasting up to half a minute with precise camera control and seamless loop closure in 3D. We highlight these results in Fig. 1 and showcase more examples of camera control in Fig. 2.

To achieve this, we carefully design our pipeline in three key aspects: model design, training recipe, and sampling method at inference. First, S EVA avoids explicit 3D representations within the network, allowing the model to inherit strong priors from pre-trained 2D models. Second, during training, we carefully craft our view selection strategy to cover both small and large viewpoint changes, ensuring strong generalization to diverse NVS tasks. Third, at inference, we introduce a two-pass procedural sampling approach that supports flexible input-target configurations. Together, these design choices create a versatile 3D “virtual camera simulation system” capable of synthesizing novel views along arbitrary camera trajectories with any number of input and target views, without using a 3D representation.

We conducted a unified benchmark across 10 datasets and a variety of experimental settings, including both opensource and proprietary models. Our benchmark reflects the diversity of real-world NVS tasks across the board and systematically evaluates existing methods beyond their comfort zones. We find that S EVA consistently outperforms previous works, achieving +1.5 dB PSNR over the state of the art CAT3D [8] in its own setup. Moreover, our method generalizes well to in-the-wild user captures, with input views ranging from 1 to 32.

In summary, our key contributions with the S EVA model include: (1) a training strategy for jointly modeling large viewpoint changes and temporal smoothness, (2) a two-pass procedural sampling method for smooth video generation along arbitrary long camera trajectories, (3) a comprehensive benchmark that evaluates NVS methods across different datasets and settings, and (4) an open-source release of model weights to support future research.

## 2. Background

We consider the evaluation of an NVS model across three key criteria: (1) generation capacity—the ability to synthesize missing regions for large viewpoint changes; (2) interpolation smoothness—the ability to produce seamless transitions between views; and (3) input flexibility—the ability to handle a variable number of input and target views.

### 2.1. Types of NVS Tasks

Given M input view images Iinp ∈ RM ×H×W ×3 of H × W resolution, along with their corresponding cameras π inp , NVS involves predicting N targets views Itgt ∈ RN ×H×W ×3 , specified by their respective cameras π tgt . For each camera, we assume we know both intrinsics and extrinsics. Based on the number of input views, we define the “sparse-view regime” as having up to 8 input views, and the “semi-dense-view regime” as an intermediate state bridging the sparse-view regime and dense captures, which typically involve hundreds of views. Based on the nature of their target views, we bucket a broad range of NVS tasks into “set NVS” and “trajectory NVS”, as shown in Fig. 3.

Set NVS considers a set of target views in arbitrary order, usually across a large spatial range. The order of views is often not helpful here, and a good NVS model requires great generation capacity to excel at this task. We note that some works address only this task (e.g. ReconFusion [7]).

Trajectory NVS regards target views along a smooth camera trajectory, such that they form a video sequence. However, they are often sampled within a small spatial range in a shorter video. To solve this task, a good NVS model requires great interpolation smoothness to produce consistent and non-flickering results. We note that some existing works address only this task (e.g. ViewCrafter [9]).

### 2.2. Existing Models

We group existing approaches into regression- and diffusion-based models based on their high-level design choices. A more detailed discussion of related works can be found in Appendix B.

Regression-based models learn a deterministic mapping to directly generate Itgt deterministically from Iinp , π inp , π tgt . fθ can be either an end-to-end network parameterized by θ, or a composition of a feed-forward prediction of an intermediate 3D representation and then a neural renderer (e.g., NeRF [20] or 3DGS [3]). For the latter case, set NVS and trajectory NVS are solved in the same way since there exists a persistent 3D representation.

Diffusion-based models capture the conditional distribution from which Itgt are sampled [21] iteratively. We highlight two types of models within this scope: Image and Video models. Image models are trained on unordered image sets, such that (Iinp , Itgt ) ∼ I, where I = {Iσ(1) , Iσ(2) , · · · , Iσ(M +N ) } is an image batch, and σ(·) is a random permutation function, where camera parameters are omitted for simplicity. Image models usually thrive at set NVS, but struggle in trajectory NVS since they are designed to generate images and not videos. Additionally, the unordered nature of all views solicits flexible input conditioning. Video models are instead trained on ordered views, such that (Iinp , Itgt ) ∼ V, where V = {I1 , I2 , · · · , IM +N } is a randomly sampled video batch with ordering preserved. Additional temporal operators may also be used to improve the temporal smoothness, such as temporal positional encoding and temporal attention. In contrast with image models, video models thrive at trajectory NVS, but struggle in set NVS. Moreover, all existing video models require both input and target views to be ordered (input views followed by target ones), constraining their input flexibility [10, 19, 22–24].

### 2.3. Remarks and Motivation

Existing tasks pose critical challenges to our design choices. Specifically, our design choices are made to achieve high generation capacity, smooth view interpolation, and flexible input conditioning, as compared in Tab. 1. In this way, we can employ a single model for both tasks, described next.

## 3. Method

We describe our model design and training strategy in Sec. 3.1, then our sampling process at test time in Secs. 3.2 and 3.3. A system overview is provided in Fig. 4.

### 3.1. Model Design and Training

We consider a “M -in N -out” multi-view diffusion model pθ , as notated in Sec. 2.2. We formulate this learning problem as a standard diffusion process [21] without any change.

Architecture. Our model is based on the publicly available SD 2.1 [25], which consists of an auto-encoder and a latent denoising U-Net. Following [8], we inflate the 2D self-attention of each low-resolution residual block into 3D self-attention [26] within the U-Net. To improve model capacity, we add 1D self-attention along the view axis after each self-attention block via skip connection [27, 28], bumping the model parameters from 870M to 1.3B. Optionally, we further tame this model into a video model by introducing 3D convolutions after each residual block via skip connection, similar to [22, 29], yielding 1.5B total parameters. The temporal pathway can be enabled during inference when frames within one forward pass are known to be spatially ordered, enhancing output’s smoothness.

Conditioning. To fine-tune our base model into a multiview diffusion model, we add camera conditioning as Plücker embedding [30] via concatenation [8] and adaptive layer normalization [31]. We normalize π inp and π tgt by first computing the relative pose with respect to the first input camera and then normalizing the scene scale such that all camera positions are within a [−2, 2]3 cube. For each input frame, we first encode its latent then concatenate with its Plücker embedding and a binary mask [8, 22] differentiating between input and target views. For each target frame, we use the noisy state of its latent instead. Additionally, we find it helpful [19] to also inject high-level semantic information via CLIP [32] image embedding. We zero initialize new weights for additional channels in the first layer. In our experiment, we found that our model can quickly adapt to these conditioning changes and produce realistic images with as few as 5K iterations.

Training. Let us define the training context window length T = |Iinp | + |Itgt | = M + N . One natural goal is to support large T such that we can generate a larger set of frames. However, we find that naive training is prone to divergence, and we thus employ a two-stage training curriculum. During the first stage, we train our model with T = 8 with a batch size of 1472 for 100K iterations. In the second stage, we train our model with T = 21 with a batch size of 512 for 600K iterations. Given a training video sequence, we randomly sample the number of input frames M ∈ [1, T − 1] and the frames (Iinp , Itgt ). We find it important to jointly sample I with a smaller subsampling stride to ensure sufficient temporal granularity and avoid missing critical transitions with a small probability (0.2 is used in practice). In the optional video training stage, we only train temporal weights with data sampled with a small subsampling stride and a batch size of 512 for 200K iterations. We shift the signal-to-noise ratio (SNR) in all stages as more noise is necessary to destroy the information when training with more frames, corroborating findings from [8, 33, 34]. The model is trained with squared images with H = W = 576.

### 3.2. Sampling Novel Views

Once the diffusion model is trained, we can sample it for a wide range of NVS tasks during test time. Formally, let us consider a “P -in Q-out” NVS task during testing, where we are given P = |Iinp | input frames and aim to produce Q = |Itgt | target frames. Our goal is to design a generic sampling strategy that works for all P and Q configurations, where P and Q need not be equal to M and N . We make two key observations: First, within a single forward pass, predictions are 3D consistent, provided the model is well-trained. Second, when P + Q > T , Itgt must be split into smaller chunks of size Qi such that P + Qi ⩽ T for the ith forward pass. We term this practice one-pass sampling. However, predictions across these forward passes would be inconsistent unless they share common frames to maintain local consistency within a spatial neighborhood. Building on these observations, we summarize our sampling process under two scenarios: P + Q ⩽ T and P + Q > T .

P + Q ⩽ T . We fit the task within one forward pass for simplicity and consistency. As shown in Appendix D, we find it works better to pad the forward pass to have exactly T frames by repeating the first input image, compared to changing the context window T zero-shot.

P + Q > T . We propose procedural two-pass sampling: In the first pass, we generate anchor frames Iacr using all input frames Iinp . In the second pass, we divide Itgt into chunks and generate them using Iacr (and optionally Iinp ) according to the spatial distribution of Iacr and Itgt . Given the distinct nature of the two tasks of interest—set NVS and trajectory NVS—e.g., differences in the availability of views’ ordering, we design tailored chunking strategies for each task.

For set NVS, we consider nearest procedural sampling. We first generate Iacr based on pre-defined trajectory priors, similar to [8], e.g., 360 trajectories for object-centric scenes, or spiral trajectories for forward-facing scenes. We then divide Itgt into chunks w.r.t. Iacr using nearest neighbor.

For trajectory NVS, we consider interp procedural sampling. We first generate a subset of target frames as Iacr by uniformly spanning the target camera path with a stride. We then generate the rest of Itgt as segments between those anchors.

### 3.3. Scaling Sampling for Large P and Q

Next, we examine two special cases when P + Q > T : P > T and Q ≫ T . Here, we make a tailored design for anchor generation in the first pass, while keeping target generation in the second pass unchanged.

P > T . In the semi-dense-view regime (e.g., P = 32), we extend the context window length T zero-shot to accommodate all P input views and anchor views in one pass during anchor generation.

Q ≫ T . When the number of target views Q is large, e.g., in large-set NVS or long-trajectory NVS, even anchors will be chunked into different forwards in the first pass, leading to the inconsistency of anchors. To this end, we maintain a memory bank of anchor views previously generated. We generate anchors autoregressively by retrieving their spatially nearest ones from the memory bank.

## 4. Experiments

We employ a single model for a spectrum of settings and find that S EVA model generalizes well under the three criteria (Tab. 1). We cover different NVS tasks (set NVS and trajectory NVS) and examine one special task of interest—long trajectory NVS. We also cover different input regimes (single-view, sparse-view, and semi-dense-view).

### 4.1. Benchmark

We consider 10 datasets and a variety of experimental settings, including both open-source and proprietary models. Our benchmark reflects the diversity of real-world NVS tasks.

### 4.2. Set NVS

Quantitative comparison. S EVA sets state-of-the-art results in the majority of splits. In the sparse-view regime (i.e., P ⩽ 8), S EVA excels across different datasets when P > 1. In the semi-dense-view regime (e.g., P = 32), S EVA surprisingly performs favorably against specialized models. For large-viewpoint set NVS, S EVA’s quantitative advantages are even more prominent.

Qualitative comparison. For small-viewpoint set NVS, the output from S EVA with the best scale exhibits desirable alignment with the ground truth while being more photorealistic in details. For large-viewpoint set NVS, S EVA produces reasonable results when the viewpoint change is large and excels in overall visual quality.

Comparison of 3D reconstruction. We adopt the few-view 3D reconstruction pipeline described in [8]. S EVA shows a consistent performance lead.

### 4.3. Trajectory NVS

Qualitative results. In the single-view regime (i.e., P = 1), S EVA generalizes to a wide range of images and demonstrates accurate camera-following capacity. In the sparse-view regime with few input views (i.e., 1 < P ⩽ 8), S EVA demonstrates strong generalization to in-the-wild real-world images and versatility in adapting to different numbers of input views.

Qualitative comparison. In the single-view regime (i.e., P = 1), we observe more photo-realistic and sharper output from our model. In the sparse-view regime with few input views (i.e., P = 3), our model demonstrates more photo-realistic textures.

Quantitative comparison. For small-viewpoint trajectory NVS, S EVA performs favorably against other methods. For large-viewpoint trajectory NVS with P = 3, S EVA consistency sets new state-of-the-art results.

Ablation on two-pass procedural sampling. We conduct an ablation study comparing the default interp procedural sampling with one-pass sampling and alternative procedural sampling strategies. The interp strategy drastically outperforms its counterparts in terms of temporal smoothness.

### 4.4. Long-Trajectory NVS

With the memory bank maintaining previously generated anchors, S EVA achieves robust 3D consistency for long-trajectory NVS.

### 4.5. Discussions

Zero-shot generalization of context window length T . Our model, though only trained on T = 21 frames, can generalize reasonably to larger T during sampling in the semi-dense-view regime.

Zero-shot generalization of image resolution. Our model, despite being trained only on square images with H = W = 576, generalizes well to different image resolution during sampling.

Guidance scale on generation uncertainty. The CFG scale has a significant impact on the final result. The optimal CFG scale is strongly correlated with the inherent uncertainty of the generation.

Sampling diversity of unseen areas. The model is capable of generating diverse and plausible predictions for unseen regions of input observations.

## 5. Conclusion

We present S TABLE V IRTUAL C AMERA (S EVA), a generalist diffusion model for novel view synthesis that balances large viewpoint changes and smooth interpolation while supporting flexible input and target configurations. By designing a diffusion-based architecture without 3D representation, a structured training strategy, and a two-pass procedural sampling approach, S EVA achieves 3D consistent rendering across diverse NVS tasks. Extensive benchmarking demonstrates its superiority over existing methods, with strong generalization to real-world scenes.
