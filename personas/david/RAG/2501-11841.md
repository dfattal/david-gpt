---
type: arxiv
title: "Survey on Monocular Metric Depth Estimation"
arxiv_id: "2501.11841"
authors:
  - name: "Jiuling Zhang"
    affiliation: "University of Chinese Academy of Sciences"
summary: "This survey reviews Monocular Metric Depth Estimation (MMDE), analyzing its evolution from traditional to deep learning methods, key datasets, and challenges in achieving generalizable, metric-accurate depth."
key_terms:
  - "Monocular Metric Depth Estimation (MMDE)"
  - "Zero-shot Depth Estimation"
  - "Relative Depth Estimation"
  - "Domain Generalization"
  - "Generative Diffusion Models"
  - "Patch-based Inference"
  - "Adaptive Binning"
  - "Synthetic Datasets"
  - "Depth Anything"
  - "ZoeDepth"
source_url: "https://arxiv.org/abs/2501.11841"
html_url: "https://arxiv.org/html/2501.11841"
---

## Abstract

Monocular Depth Estimation (MDE) enables spatial understanding, 3D reconstruction, and autonomous navigation, yet deep learning approaches often predict only relative depth without a consistent metric scale. This limitation reduces reliability in applications such as visual SLAM, precise 3D modeling, and view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this challenge by producing depth maps with absolute scale, ensuring geometric consistency and enabling deployment without additional calibration. This survey reviews the evolution of MMDE, from geometry-based methods to state-of-the-art deep models, with emphasis on the datasets that drive progress. Key benchmarks, including KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of modality, scene type, and application domain. Methodological advances are analyzed, covering domain generalization, boundary preservation, and the integration of synthetic and real data. Techniques such as unsupervised and semi-supervised learning, patch-based inference, architectural innovations, and generative modeling are evaluated for their strengths and limitations. By synthesizing current progress, highlighting the importance of high-quality datasets, and identifying open challenges, this survey provides a structured reference for advancing MMDE and supporting its adoption in real-world computer vision systems.

## 1 Preliminary
Depth estimation reconstructs 3D scene structure from images and underpins numerous applications, including 3D reconstruction [38, 25, 63], autonomous navigation [53], self-driving vehicles [68], and video understanding [29]. It also supports emerging areas such as AI-generated content (AIGC), which spans image synthesis [66, 27], video generation [33], and 3D scene reconstruction [60, 43, 45].

Classical approaches relied on parallax, stereo vision, and multi-camera systems. With advances in deep learning, monocular depth estimation (MDE) emerged as a cost-effective alternative that predicts depth from a single image. The field’s growing significance is reflected in the Monocular Depth Estimation Challenge (MDEC), hosted at CVPR in 2023 and 2024 and scheduled to return in 2025.

Recent work has shifted toward Monocular Metric Depth Estimation (MMDE), which provides absolute depth values rather than scale-inconsistent maps. This shift is driven by practical requirements for accurate, generalizable, and detail-preserving predictions in real-world tasks. Industry leaders, including Intel [4], Apple [7], DeepMind [42], TikTok [61, 62], and Bosch [19], have advanced MMDE through large-scale datasets, high-performance computing, and novel architectures, enabling improvements in zero-shot generalization and reconstruction fidelity.

However, survey literature remains limited. Most comprehensive reviews predate 2020 [5, 26, 67, 59], and recent work often focuses on domain-specific settings [28, 54, 55, 10] or relative depth [36, 1, 40]. Meanwhile, leading venues such as CVPR 2024, ECCV 2024, and NeurIPS 2024 highlight emerging trends in zero-shot MMDE and generative model integration. This paper fills a critical gap by offering a comprehensive review of MMDE, addressing datasets, methodological advances, open challenges, and future directions.

## 2 Depth Estimation
The goal of depth estimation is to compute a depth map D:=(R)^H×W from a given 2D image I:=(R)^H×W×3, where each depth value d_i,j ∈ D represents the physical distance between pixel i_i,j ∈ I and the camera [2]. This task is inherently ill-posed because 2D images are projections of the 3D world, which discard geometric information. Monocular depth estimation is particularly challenging due to the absence of parallax and auxiliary cues [37].

Despite these challenges, depth estimation plays a critical role in computer vision [23]. Accurate depth maps enhance scene understanding and object localization, which benefit a wide range of applications. In autonomous driving and robotics, they improve obstacle detection, path planning, and environmental awareness. In AR/VR, reliable depth predictions enable realistic 3D reconstruction and immersive interaction. In computational photography, depth supports multi-focus imaging, background segmentation, and 3D video synthesis [12].

By providing dense, pixel-wise distance predictions, depth estimation equips intelligent systems with geometric awareness of the environment. This capability remains a cornerstone of visual perception research with substantial real-world impact across established and emerging domains.

### 2.1 Traditional Methods
Before the rise of deep learning, depth estimation primarily relied on geometric models and specialized sensors. These approaches achieved accuracy in controlled conditions but often required additional hardware and struggled in complex real-world environments [47].

#### 2.1.1 Sensors
Early sensor-based systems directly captured spatial information. Structured-light devices, such as Microsoft Kinect v1, projected predefined patterns to infer depth, while Time-of-Flight (ToF) sensors measured light travel time. Although accurate in laboratory settings, these methods were expensive and highly sensitive to ambient light and surface properties, limiting their use in dynamic or portable applications.

#### 2.1.2 Stereo Vision
Stereo vision, inspired by human binocular perception, estimated depth from disparities between two calibrated cameras. While effective, performance degraded in low-texture regions, poor lighting, and dynamic scenes, where reliable pixel correspondence was difficult. The hardware complexity of stereo rigs further restricted widespread deployment.

#### 2.1.3 Geometric Multi-Frame Methods
Techniques such as Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) inferred depth from multi-frame parallax by estimating camera poses and reconstructing 3D point clouds. Indirect methods minimized reprojection error from feature correspondences, while direct methods exploited photometric consistency [58]. Despite enabling depth estimation without extra sensors, these methods were sensitive to illumination changes and texture inconsistencies, reducing robustness in unconstrained environments.

Traditional approaches provided a strong foundation but were limited by hardware demands, environmental constraints, and computational complexity. The shift to deep learning introduced more scalable, flexible, and robust solutions, which now dominate depth estimation research.

### 2.2 Deep Learning
Deep learning has fundamentally reshaped depth estimation, replacing geometry-based methods with learning-driven approaches. Unlike stereo or LiDAR-based systems, neural networks predict depth directly from a single image, reducing hardware cost and enabling lightweight deployment in applications such as mobile AR and drone navigation [16].

A major advantage of deep learning is its ability to leverage large-scale datasets to capture scene priors that resolve the inherent ambiguity of monocular input. Neural networks learn both local textures and global semantics, allowing inference of spatial relationships and geometric structure. For example, sky regions are recognized as distant, while ground-plane textures provide depth gradients, leading to more reliable predictions even in ill-posed regions.

Feature representation is central to modern models. Convolution Neural Networks (CNNs) extract multi-scale information, integrating fine textures with high-level semantics to improve precision and robustness in structured environments. This feature-driven paradigm surpasses pixel-based geometric methods by achieving reliable performance across sparse-texture, cluttered, and dynamic scenes.

Deep learning thus enables scalable, cost-efficient, and robust monocular depth estimation, expanding its impact on domains such as autonomous driving, robotics, and immersive AR/VR systems.

## 3 Monocular Depth Estimation
Monocular Depth Estimation (MDE) predicts scene depth from a single RGB image, eliminating the need for multi-view setups or specialized sensors. Neural networks extract visual cues directly, thereby reducing system complexity and cost compared to traditional geometry-based methods.

Early supervised approaches demonstrated the feasibility of learning depth from labeled datasets. Eigen et al. introduced a multi-scale CNN that jointly predicted global and local depth, significantly improving accuracy [12]. Their subsequent work incorporated surface normals and semantic labels in a multi-task framework, further enhancing robustness [11].

Modern architectures largely adopt encoder–decoder designs, where encoders capture global context and decoders reconstruct fine-grained depth. Multi-scale feature fusion strengthens the balance between structure and detail. To mitigate the intrinsic ambiguity of monocular cues, several works integrate geometric priors such as perspective constraints and object size, improving plausibility and generalization.

Although early models often struggled with cross-domain transfer, advances in universal feature extraction and domain-invariant learning have expanded MDE’s applicability in real-world scenarios. These developments establish MDE as a practical and scalable solution for tasks requiring dense depth perception.

## 4 Zero-shot Monocular Depth Estimation
Zero-shot MDE addresses the poor transferability of supervised models, which often rely on dataset-specific scales and camera intrinsics. Early solutions reformulated the task as Relative Depth Estimation (RDE), predicting ordinal pixel relationships rather than absolute distances. This scale-agnostic formulation, coupled with scale-invariant and scale-and-shift-invariant loss functions, improved generalization across heterogeneous datasets [13].

A major milestone was MiDaS [6], which unified multi-dataset training under scale-invariant objectives. By evolving from CNN-based designs to Vision Transformers [21], MiDaS demonstrated strong cross-domain performance, establishing a foundation for zero-shot depth prediction. However, RDE inherently trades metric precision for generalization: while robust across domains, it cannot provide absolute scale, limiting applications such as SLAM, AR, and autonomous driving where metric consistency and temporal coherence are critical.

Recent research aims to bridge relative and metric estimation within unified frameworks, seeking to balance robustness with scale fidelity. These efforts mark a shift toward zero-shot models that can generalize across domains while remaining suitable for real-world deployment.

## 5 Monocular Metric Depth Estimation
Monocular Metric Depth Estimation (MMDE) has gained prominence due to its ability to predict absolute depth in physical units, enabling consistent 3D perception for applications such as reconstruction, novel view synthesis, and SLAM. Unlike relative methods, MMDE ensures geometric stability and temporal coherence across frames, making it more practical for real-world deployment.

Early approaches assumed known camera intrinsics. Metric3D mapped images to a canonical space with focal length corrections [64], while ZeroDepth leveraged variational inference with camera-specific embeddings [18]. More recent work removes this dependency by estimating intrinsics through auxiliary networks or predicting depth in spherical representations [49].

Advances in adaptive binning have further improved accuracy. AdaBins introduced dynamic depth bin allocation [2], refined by LocalBins through spatial partitioning [3], while BinsFormer integrated Transformers for global-local bin optimization [32]. NeW CRFs combined neural networks with conditional random fields to enforce pixel-wise consistency [65].

A breakthrough in zero-shot MMDE was ZoeDepth [4], which extended MiDaS with adaptive metric binning and scene-aware routing, achieving strong cross-domain generalization across indoor and outdoor datasets. This unified design has established a benchmark for scalable and robust metric depth estimation.

## 6 Challenges and Improvements
Although MMDE has advanced significantly, generalization to unseen domains remains a primary challenge [50]. Models often suffer from geometric blurring, loss of fine details, and degraded performance in high-resolution or cross-domain scenarios, limiting reliability in real-world applications.

Recent improvements target these issues through architectural refinements, such as multi-scale feature fusion and transformer-based designs, which better preserve structure and context. Enhanced training strategies, including domain-invariant learning and large-scale multi-dataset supervision, improve robustness across diverse environments. At inference, patch-based and adaptive mechanisms mitigate resolution constraints while maintaining geometric consistency. Collectively, these advances have boosted prediction accuracy and stability, yet achieving both high precision and strong generalization in dynamic settings remains an open challenge for MMDE.

### 6.1 Generalizability
Improving the generalization of zero-shot MMDE relies on large-scale data augmentation, robust architectures, and novel training paradigms.

Dataset Augmentation: Depth Anything employs a semi-supervised framework generating 62M self-annotated images, enabling strong cross-domain adaptation through semantic priors and large-scale supervision [61, 35, 20, 44, 57]. Depth Any Camera (DAC) further extends depth estimation to fisheye and 360° imagery using ERP, pitch-aware conversion, and field-of-view alignment, achieving robust omnidirectional prediction [19].

Model Improvements: UniDepth predicts metric 3D point clouds without camera intrinsics via a self-promptable camera module, pseudo-spherical outputs, and geometric invariance loss, enhancing robustness to camera variations and domain shifts [39].

Loss and Training Paradigms: DepthAnything-AC introduces unsupervised consistency regularization and a Spatial Distance Constraint, reducing noise sensitivity and improving fine-structure preservation in degraded conditions [51].

### 6.2 Blurriness
Depth estimation models often suffer from blurred edges and loss of fine details, particularly around object boundaries, occlusions, and high-frequency textures. This degradation reduces structural accuracy and limits applicability in high-precision tasks.

Patch-based methods improve detail by combining local and global cues. BoostingDepth and PatchFusion use multi-resolution fusion to enhance sharpness, though at high computational cost [37, 30]. PatchRefiner introduces Detail and Scale Disentangling (DSD) loss and pseudo-labeling to sharpen boundaries while improving efficiency [31]. DepthPro further balances detail and speed through a multi-scale Vision Transformer with patch slicing, though with reduced accuracy in distant regions [7].

Synthetic datasets provide pixel-accurate labels for training, reducing boundary artifacts. Depth Anything V2 leverages synthetic supervision with pseudo-labeling and gradient-matching loss to bridge domain gaps, enhancing fine detail and robustness [62, 31].

Generative diffusion methods restore structural fidelity through progressive refinement. Marigold achieves sharper predictions in reflective and transparent regions [24], while GeoWizard introduces scene-aware decoupling and normal map integration for improved 3D geometry [14]. DeepMind’s DMD applies logarithmic depth parameterization and FOV conditioning to resolve scale ambiguities and accelerate inference [42].

### 6.3 Incremental Sensor Assistance
Lightweight sensor cues have been explored as auxiliary signals to improve monocular metric depth estimation. Lin et al. [34] propose incorporating sparse LiDAR data as incremental prompts within a depth decoder, guiding pre-trained foundation models toward more accurate and high-resolution metric predictions. This hybrid strategy leverages minimal sensor input while retaining the scalability of purely vision-based approaches.

Table 1: Timeline of key advancements in monocular metric depth estimation (MMDE). While most generative approaches focus on relative depth, Diffusion for Metric Depth (DMD) remains the only reported method producing absolute metric predictions [42]. However, the absence of public release limits independent validation, leaving the role of generative models in MMDE an open avenue for future exploration.

Table 2: Comparative evaluation of monocular metric depth estimation (MMDE) models. ZeroDepth is limited by storage, Metric3D depends on camera parameters, and Depth Anything underperforms in zero-shot generalization. Results across six zero-shot (higher-is-better) and two non-zero-shot (AbsRel, lower-is-better) benchmarks, reported by DepthPro [7], reveal substantial performance variation. The absence of standardized datasets, training protocols, and model settings continues to hinder fair cross-model comparison.

### 6.4 Analysis and Comparison
Single-inference methods dominate monocular depth estimation due to their efficiency, producing depth in a single forward pass suitable for real-time tasks such as navigation and view synthesis. However, these models often lose fine structural details and rely heavily on large-scale, high-quality annotations, limiting generalization in noisy real-world datasets.

Patch-based strategies improve resolution by processing local regions independently and fusing results, enabling finer structural recovery. Methods like PatchFusion enhance accuracy via optimized patch weighting, but inference scales with patch count, causing latency that restricts deployment in high-resolution or time-critical scenarios [30].

Figure 1: Performance comparison of MMDE models across diverse scenarios (indoor/outdoor, urban/natural, large/small scale, varying illumination). Colors indicate scene types and method categories. Generative methods predict relative depth, while others yield absolute depth [7].

Figure 2: Inference time and memory usage for different model types are shown on a logarithmic scale in seconds.

Generative diffusion models offer an alternative by progressively refining depth predictions, capturing complex geometries with strong structural consistency [24]. While effective in detail preservation and less reliant on labeled data, their multi-step denoising introduces computational overhead and variability. Most focus on relative depth, with limited exploration of monocular metric depth estimation (MMDE). Notably, DMD incorporates field-of-view conditioning and log-scale parameterization to achieve accurate zero-shot MMDE [42], though inference inefficiency remains a bottleneck.

In summary, single-inference methods trade detail for speed, patch-based methods improve accuracy at high cost, and generative models offer superior fidelity but face scalability challenges.

## 7 Datasets for MMDE
A wide range of datasets, as shown in Table 3, supports monocular metric depth estimation (MMDE), with outdoor collections dominating due to their relevance to autonomous driving. Benchmarks such as KITTI, Waymo Open Dataset, and nuScenes provide rich multi-sensor data—including RGB, LiDAR, and radar—offering reliable metric depth for training and evaluation [17, 8, 52].

Indoor datasets, though fewer, are vital for robotics and AR applications. Notable examples include ScanNet, NYU Depth V2, and SUN RGB-D, which supply high-quality RGB-D imagery and accurate ground truth [9, 46, 48].

Synthetic datasets such as TartanAir, Hypersim, and vKITTI provide pixel-perfect, noise-free depth maps, complementing real-world datasets by enabling large-scale, diverse, and controlled training scenarios [56, 41, 15]. These resources are increasingly leveraged to mitigate annotation errors and domain gaps in real-world data.

Dataset diversity also extends to sensor configurations, from RGB-only captures to multi-modal setups integrating LiDAR, GPS, and IMU. Crucially, most reviewed datasets (32 of 38) provide true metric depth, while a smaller subset offers only relative depth, which remains useful in scale-agnostic applications.

In summary, real-world driving datasets dominate current MMDE research, indoor datasets provide indispensable complementary supervision, and synthetic datasets enhance diversity and precision, together forming the backbone of benchmarking and training.

## 8 Summary and Outlook
MMDE has evolved from traditional architectures toward generative modeling and domain-generalizable frameworks. Tables 1 and 2 highlight art methods and performance benchmarks, illustrating progress in both architectural design and data utilization. These advances have broadened the applicability to 3D reconstruction, navigation, and interactive perception. Nonetheless, challenges remain, including fine-detail preservation, geometric consistency in complex scenes, and the trade-off between accuracy and efficiency [50, 61].

Loss design continues to play a critical role, with edge-aware and gradient-based formulations improving structural fidelity, while generative supervision further enhances robustness [51]. Data strategies combining real-world and synthetic datasets [56, 41] mitigate annotation scarcity and domain gaps, offering a scalable foundation for generalization.

Generative diffusion models, such as Marigold and GeoWizard, show strong potential in recovering high-frequency details and complex geometries, while DMD introduces field-of-view conditioning and log-scale parameterization for improved adaptability [7]. Although diffusion methods remain computationally demanding, optimization of multi-step inference is advancing toward practical deployment.

A clear research trend is zero-shot generalization across unseen domains. Approaches like ZoeDepth and UniDepth [39] demonstrate promising transferability through architectural innovations and large-scale training. Looking forward, priorities include improving computational efficiency, enforcing geometric consistency in multi-view settings, and advancing domain adaptation.

MMDE is moving toward becoming a cornerstone of spatial perception, with ongoing innovations in loss functions, hybrid data pipelines, and generative modeling steadily pushing the field toward universal, accurate, and efficient depth estimation.

Table 3: Overview of depth estimation datasets commonly used in computer vision. Datasets providing metric depth (e.g., NYU-D, KITTI, ApolloScape) supply RGB–depth pairs for supervised MMDE, while relative depth datasets (e.g., DIW, Movies, WSVD) support ordinal depth prediction. Synthetic and hybrid datasets (e.g., BlendedMVS, TartanAir [56]) augment real-world data and aid domain adaptation. Columns summarize dataset name, scene type (indoor/outdoor), driving relevance, synthetic/real origin, supported tasks, available modalities, supervision type, and key features, facilitating selection for applications ranging from autonomous driving to indoor scene understanding.