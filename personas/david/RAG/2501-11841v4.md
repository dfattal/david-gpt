---
id: 2501-11841v4
title: Survey on Monocular Metric Depth Estimation
date: 2025-01-21
source_url: https://arxiv.org/abs/2501.11841
type: arxiv
personas: [david]
summary: This survey reviews the evolution of Monocular Metric Depth Estimation (MMDE), from geometry-based methods to state-of-the-art deep models, with emphasis on the datasets that drive progress.
identifiers:
  arxiv_id: "2501.11841"
  doi: "10.48550/arXiv.2501.11841"
dates:
  submitted: "2025-01-21"
  updated: "2025-08-26"
  published: null
actors:
  - name: "Jiuling Zhang"
    role: "author"
---

**Key Terms**: Monocular Depth Estimation (MDE), Monocular Metric Depth Estimation (MMDE), Relative Depth Estimation (RDE), Computer Vision (CV), SLAM, 3D Reconstruction, Deep Learning, Neural Networks, CNN, Vision Transformers, Zero-shot Learning, Generative Models, Diffusion Models, LiDAR, RGB-D, Autonomous Driving, Augmented Reality (AR), Virtual Reality (VR).
**Also Known As**: MDE, MMDE, RDE.

## Abstract

Monocular Depth Estimation (MDE) enables spatial understanding, 3D reconstruction, and autonomous navigation, yet deep learning approaches often predict only relative depth without a consistent metric scale. This limitation reduces reliability in applications such as visual SLAM, precise 3D modeling, and view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this challenge by producing depth maps with absolute scale, ensuring geometric consistency and enabling deployment without additional calibration. This survey reviews the evolution of MMDE, from geometry-based methods to state-of-the-art deep models, with emphasis on the datasets that drive progress. Key benchmarks, including KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of modality, scene type, and application domain. Methodological advances are analyzed, covering domain generalization, boundary preservation, and the integration of synthetic and real data. Techniques such as unsupervised and semi-supervised learning, patch-based inference, architectural innovations, and generative modeling are evaluated for their strengths and limitations. By synthesizing current progress, highlighting the importance of high-quality datasets, and identifying open challenges, this survey provides a structured reference for advancing MMDE and supporting its adoption in real-world computer vision systems.

## Introduction

Depth estimation reconstructs 3D scene structure from images and underpins numerous applications, including 3D reconstruction, autonomous navigation, self-driving vehicles, and video understanding. It also supports emerging areas such as AI-generated content (AIGC), which spans image synthesis, video generation, and 3D scene reconstruction. Classical approaches relied on parallax, stereo vision, and multi-camera systems. With advances in deep learning, monocular depth estimation (MDE) emerged as a cost-effective alternative that predicts depth from a single image. The field’s growing significance is reflected in the Monocular Depth Estimation Challenge (MDEC), hosted at CVPR.

Recent work has shifted toward Monocular Metric Depth Estimation (MMDE), which provides absolute depth values rather than scale-inconsistent maps. This shift is driven by practical requirements for accurate, generalizable, and detail-preserving predictions in real-world tasks. Industry leaders have advanced MMDE through large-scale datasets, high-performance computing, and novel architectures, enabling improvements in zero-shot generalization and reconstruction fidelity. However, comprehensive survey literature is limited, with most reviews predating recent advances in zero-shot MMDE and generative model integration. This paper fills a critical gap by offering a comprehensive review of MMDE, addressing datasets, methodological advances, open challenges, and future directions.

The goal of depth estimation is to compute a depth map from a given 2D image, where each depth value represents the physical distance between a pixel and the camera. This task is inherently ill-posed because 2D images are projections of the 3D world, which discard geometric information. Monocular depth estimation is particularly challenging due to the absence of parallax and auxiliary cues. Despite these challenges, accurate depth maps enhance scene understanding and object localization, benefiting a wide range of applications from autonomous driving to AR/VR and computational photography.

## Methodology

### Traditional Methods
Before the rise of deep learning, depth estimation primarily relied on geometric models and specialized sensors. Early sensor-based systems like Structured-light devices (Microsoft Kinect v1) and Time-of-Flight (ToF) sensors directly captured spatial information but were expensive and sensitive to environmental conditions. Stereo vision estimated depth from disparities between two calibrated cameras but struggled in low-texture regions and dynamic scenes. Geometric multi-frame methods like Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) inferred depth from multi-frame parallax but were sensitive to illumination and texture inconsistencies. These traditional approaches were limited by hardware demands, environmental constraints, and computational complexity.

### Deep Learning Approaches
Deep learning has reshaped depth estimation by predicting depth directly from a single image, reducing hardware cost and leveraging large-scale datasets to capture scene priors. Neural networks learn both local textures and global semantics, allowing for reliable predictions even in ill-posed regions. Convolutional Neural Networks (CNNs) extract multi-scale information, integrating fine textures with high-level semantics. This feature-driven paradigm surpasses pixel-based geometric methods by achieving reliable performance across diverse scenes.

### Monocular and Zero-Shot Estimation
Monocular Depth Estimation (MDE) predicts scene depth from a single RGB image. Early supervised approaches used multi-scale CNNs to jointly predict global and local depth. Modern architectures often adopt encoder–decoder designs with multi-scale feature fusion. To mitigate ambiguity, some works integrate geometric priors like perspective constraints.

Zero-shot MDE addresses the poor transferability of supervised models. Early solutions reformulated the task as Relative Depth Estimation (RDE), predicting ordinal pixel relationships. A major milestone was MiDaS, which unified multi-dataset training under scale-invariant objectives and used Vision Transformers to achieve strong cross-domain performance. However, RDE trades metric precision for generalization, as it cannot provide absolute scale.

### Monocular Metric Depth Estimation (MMDE)
MMDE predicts absolute depth in physical units, ensuring geometric stability and temporal coherence. Early approaches assumed known camera intrinsics, but more recent work removes this dependency by estimating intrinsics or using spherical representations. Advances in adaptive binning have improved accuracy, with methods like AdaBins, LocalBins, and BinsFormer introducing dynamic and optimized bin allocation. A breakthrough in zero-shot MMDE was ZoeDepth, which extended MiDaS with adaptive metric binning and scene-aware routing, achieving strong cross-domain generalization.

## Results

### Challenges and Improvements
Despite significant advances, MMDE models still face challenges in generalization, geometric blurring, and loss of fine details. Recent improvements target these issues through architectural refinements, enhanced training strategies, and advanced inference techniques.

**Generalizability**: Improving generalization relies on large-scale data augmentation, robust architectures, and novel training paradigms. **Depth Anything** employs a semi-supervised framework with 62M self-annotated images. **Depth Any Camera (DAC)** extends estimation to fisheye and 360° imagery. **UniDepth** predicts metric 3D point clouds without camera intrinsics, enhancing robustness to camera variations.

**Blurriness**: To combat blurred edges and loss of detail, patch-based methods like **BoostingDepth** and **PatchFusion** use multi-resolution fusion. **PatchRefiner** introduces a Detail and Scale Disentangling (DSD) loss. Synthetic datasets are leveraged by **Depth Anything V2** to enhance fine detail. Generative diffusion methods like **Marigold** and **GeoWizard** restore structural fidelity through progressive refinement.

**Incremental Sensor Assistance**: Some approaches incorporate sparse LiDAR data as incremental prompts within a depth decoder to guide pre-trained models toward more accurate metric predictions, retaining the scalability of vision-based methods.

### Analysis and Comparison
Single-inference methods are efficient and suitable for real-time tasks but can lose fine details. Patch-based strategies improve resolution and structural recovery but at a higher computational cost. Generative diffusion models offer superior fidelity and are less reliant on labeled data, but their multi-step inference process introduces overhead. While most generative models focus on relative depth, **DMD** from DeepMind is a notable exception that produces absolute metric predictions, though its inference inefficiency remains a bottleneck.

The table below provides a comparative evaluation of various MMDE models across several benchmarks. Performance varies substantially, and the lack of standardized datasets and training protocols hinders fair cross-model comparison.

| Method | Booster↑ indoor | ETH3D↑ outdoor | Middlebury↑ outdoor | NuScenes↑ outdoor | Sintel↑ outdoor | Sun-RGBD↑ indoor | NYU v2↓ indoor | KITTI↓ outdoor |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| DepthAnything | 52.3 | 9.3 | 39.3 | 35.4 | 6.9 | 85.0 | 4.3 | 7.6 |
| DepthAnything V2 | 59.5 | 36.3 | 37.2 | 17.7 | 5.9 | 72.4 | 4.4 | 7.4 |
| Metric3D | 4.7 | 34.2 | 13.6 | 64.4 | 17.3 | 16.9 | 8.3 | 5.8 |
| Metric3D v2 | 39.4 | 87.7 | 29.9 | 82.6 | 38.3 | 75.6 | 4.5 | 3.9 |
| PatchFusion | 22.6 | 51.8 | 49.9 | 20.4 | 14.0 | 53.6 | 5.78 | 4.2 |
| UniDepth | 27.6 | 25.3 | 31.9 | 83.6 | 16.5 | 95.8 | 8.4 | 10.5 |
| ZeroDepth | 21.6 | 34.2 | 46.5 | 64.3 | 12.9 | 85.7 | 7.7 | 5.7 |
| ZoeDepth | 46.6 | 41.5 | 53.8 | 28.1 | 7.8 | 89.0 | - | - |
| Depth Pro | | | 60.5 | 49.1 | 40.0 | | | |

## Discussion

### Datasets for MMDE
A wide range of datasets supports MMDE research, with outdoor collections dominating due to their relevance to autonomous driving. Benchmarks like **KITTI**, **Waymo Open Dataset**, and **nuScenes** provide rich multi-sensor data (RGB, LiDAR, radar) for reliable metric depth. Indoor datasets such as **ScanNet**, **NYU Depth V2**, and **SUN RGB-D** are vital for robotics and AR applications, supplying high-quality RGB-D imagery.

Synthetic datasets like **TartanAir**, **Hypersim**, and **vKITTI** provide pixel-perfect, noise-free depth maps, complementing real-world data by enabling large-scale and controlled training scenarios. These resources are increasingly leveraged to mitigate annotation errors and domain gaps. The diversity of datasets extends to sensor configurations, from RGB-only to multi-modal setups with LiDAR, GPS, and IMU. The majority of reviewed datasets provide true metric depth, which is crucial for supervised MMDE.

## Conclusion

MMDE has evolved from traditional architectures toward generative modeling and domain-generalizable frameworks. Advances in architectural design and data utilization have broadened its applicability to 3D reconstruction, navigation, and interactive perception. However, challenges in fine-detail preservation, geometric consistency, and the accuracy-efficiency trade-off remain.

Loss design continues to be critical, with edge-aware and gradient-based formulations improving structural fidelity. Data strategies combining real and synthetic datasets mitigate annotation scarcity and domain gaps. Generative diffusion models show strong potential in recovering high-frequency details, though they remain computationally demanding.

A clear research trend is zero-shot generalization across unseen domains. Approaches like ZoeDepth and UniDepth demonstrate promising transferability. Future priorities include improving computational efficiency, enforcing geometric consistency in multi-view settings, and advancing domain adaptation. MMDE is moving toward becoming a cornerstone of spatial perception, with ongoing innovations steadily pushing the field toward universal, accurate, and efficient depth estimation.
