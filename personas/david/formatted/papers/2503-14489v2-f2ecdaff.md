---
title: "S TABLE V IRTUAL C AMERA: Generative View Synthesis with Diffusion Models"
docType: paper
url: https://arxiv.org/abs/2503.14489v2
scraped_at: "2025-09-26T05:56:32.009Z"
word_count: 6983
extraction_quality: high
authors:
  - name: "Jensen (Jinghao) Zhou"
    affiliation: "Stability AI, University of Oxford"
  - name: "Hang Gao"
    affiliation: "Stability AI, University of California, Berkeley"
  - name: "Vikram Voleti"
    affiliation: "Stability AI"
  - name: "Aaryaman Vasishta"
    affiliation: "Stability AI"
  - name: "Chun-Han Yao"
    affiliation: "Stability AI"
  - name: "Mark Boss"
    affiliation: "Stability AI"
  - name: "Philip Torr"
    affiliation: "University of Oxford"
  - name: "Christian Rupprecht"
    affiliation: "University of Oxford"
  - name: "Varun Jampani"
    affiliation: "Stability AI"
venue: "arXiv"
publicationYear: 2025
abstract: "We present S TABLE V IRTUAL C AMERA (S EVA), a generalist diffusion model that creates novel views of a scene, given any number of input views and target cameras. Existing works struggle to generate either large viewpoint changes or temporally smooth samples, while relying on specific task configurations. Our approach overcomes these limitations through simple model design, optimized training recipe, and flexible sampling strategy that generalize across view synthesis tasks at test time. As a result, our samples maintain high consistency without requiring additional 3D representation-based distillation, thus streamlining view synthesis in the wild. Furthermore, we show that our method can generate high-quality videos lasting up to half a minute with seamless loop closure. Extensive benchmarking demonstrates that S EVA outperforms existing methods across different datasets and settings."
keywords: ["novel view synthesis", "diffusion models", "generative models", "3D consistency", "video generation"]
technologies: ["lightfield", "autostereoscopic displays", "holography", "computer vision"]
---

# Abstract

We present S TABLE V IRTUAL C AMERA (S EVA), a generalist diffusion model that creates novel views of a scene, given any number of input views and target cameras. Existing works struggle to generate either large viewpoint changes or temporally smooth samples, while relying on specific task configurations. Our approach overcomes these limitations through simple model design, optimized training recipe, and flexible sampling strategy that generalize across view synthesis tasks at test time. As a result, our samples maintain high consistency without requiring additional 3D representation-based distillation, thus streamlining view synthesis in the wild. Furthermore, we show that our method can generate high-quality videos lasting up to half a minute with seamless loop closure. Extensive benchmarking demonstrates that S EVA outperforms existing methods across different datasets and settings.

# 1. Introduction

Novel view synthesis (NVS) aims to generate realistic, 3Dconsistent images of a scene from arbitrary camera viewpoints given any number of camera-posed input views. Traditional methods, which rely on dense input views, treat NVS as a 3D reconstruction and rendering problem [1– 3], but this approach fails with sparse inputs. Generative view synthesis addresses this limitation by leveraging modern deep network priors [4, 5], enabling immersive 3D interactions in uncontrolled environments without the need to capture large image sets per scene. In this work, we focus on generative view synthesis and, unless otherwise specified, refer to it simply as NVS for clarity.

Despite recent progress [6–12], NVS in the wild remains limited due to two key challenges: First, existing methods struggle to generate both large viewpoint changes [9, 10] and temporally smooth samples [6–8, 13] while being constrained by rigid task configurations, such as a fixed number of input and target views [7, 9, 11, 12], reviewed in Tab. 1. Second, their sampling consistency is often insufficient, necessitating additional NeRF distillation to fuse inconsistent results into a coherent representation [7, 8, 13]. These limitations hinder their applicability across diverse NVS tasks, which we address in this work.

We present S TABLE V IRTUAL C AMERA1 (S EVA), a diffusion-based NVS model that generalizes across a spectrum of view synthesis tasks without requiring NeRF distillation. With a single network, S EVA generates high-quality novel views that strike both large viewpoint changes and temporal smoothness, while supporting any number of input and target views. Our approach simplifies the NVS pipeline without requiring distillation from a 3D representation, thus streamlining it for real-world applications. For the first time, we demonstrate high-quality videos lasting up to half a minute with precise camera control and seamless loop closure in 3D. We highlight these results in Fig. 1 and showcase more examples of camera control in Fig. 2.

To achieve this, we carefully design our pipeline in three key aspects: model design, training recipe, and sampling method at inference. First, S EVA avoids explicit 3D representations within the network, allowing the model to inherit strong priors from pre-trained 2D models. Second, during training, we carefully craft our view selection strategy to cover both small and large viewpoint changes, ensuring strong generalization to diverse NVS tasks. Third, at inference, we introduce a two-pass procedural sampling approach that supports flexible input-target configurations. Together, these design choices create a versatile 3D “virtual camera simulation system” capable of synthesizing novel views along arbitrary camera trajectories with any number of input and target views, without using a 3D representation.

We conducted a unified benchmark across 10 datasets and a variety of experimental settings, including both opensource and proprietary models. Our benchmark reflects the diversity of real-world NVS tasks across the board and systematically evaluates existing methods beyond their comfort zones. We find that S EVA consistently outperforms previous works, achieving +1.5 dB PSNR over the state of the art CAT3D [8] in its own setup. Moreover, our method generalizes well to in-the-wild user captures, with input views ranging from 1 to 32.

In summary, our key contributions with the S EVA model include: (1) a training strategy for jointly modeling large viewpoint changes and temporal smoothness, (2) a two-pass procedural sampling method for smooth video generation along arbitrary long camera trajectories, (3) a comprehensive benchmark that evaluates NVS methods across different datasets and settings, and (4) an open-source release of model weights to support future research.

# 2. Background

We consider the evaluation of an NVS model across three key criteria: (1) generation capacity—the ability to synthesize missing regions for large viewpoint changes; (2) interpolation smoothness—the ability to produce seamless transitions between views; and (3) input flexibility—the ability to handle a variable number of input and target views; We review existing NVS models based on these criteria in Tab. 1, including the types of training data.

## 2.1. Types of NVS Tasks

Given M input view images Iinp ∈ RM ×H×W ×3 of H × W resolution, along with their corresponding cameras π inp , NVS involves predicting N targets views Itgt ∈ RN ×H×W ×3 , specified by their respective cameras π tgt . For each camera, we assume we know both intrinsics and extrinsics. Based on the number of input views, we define the “sparse-view regime” as having up to 8 input views, and the “semi-dense-view regime” as an intermediate state bridging the sparse-view regime and dense captures, which typically involve hundreds of views. Based on the nature of their target views, we bucket a broad range of NVS tasks into “set NVS” and “trajectory NVS”, as shown in Fig. 3.

Set NVS considers a set of target views in arbitrary order, usually across a large spatial range. The order of views is often not helpful here, and a good NVS model requires great generation capacity to excel at this task. We note that some works address only this task (e.g. ReconFusion [7]).

Trajectory NVS regards target views along a smooth camera trajectory, such that they form a video sequence. However, they are often sampled within a small spatial range in a shorter video. To solve this task, a good NVS model requires great interpolation smoothness to produce consistent and non-flickering results. We note that some existing works address only this task (e.g. ViewCrafter [9]).

## 2.2. Existing Models

We group existing approaches into regression- and diffusion-based models based on their high-level design choices. A more detailed discussion of related works can be found in Appendix B.

Regression-based models learn a deterministic mapping to directly generate Itgt deterministically from Iinp , π inp , π tgt . fθ can be either an end-to-end network parameterized by θ, or a composition of a feed-forward prediction of an intermediate 3D representation and then a neural renderer (e.g., NeRF [20] or 3DGS [3]). For the latter case, set NVS and trajectory NVS are solved in the same way since there exists a persistent 3D representation.

Diffusion-based models capture the conditional distribution from which Itgt are sampled [21] iteratively. We highlight two types of models within this scope: Image and Video models. Image models are trained on unordered image sets, such that (Iinp , Itgt ) ∼ I, where I = {Iσ(1) , Iσ(2) , · · · , Iσ(M +N ) } is an image batch, and σ(·) is a random permutation function, where camera parameters are omitted for simplicity. Image models usually thrive at set NVS, but struggle in trajectory NVS since they are designed to generate images and not videos. Additionally, the unordered nature of all views solicits flexible input conditioning. Video models are instead trained on ordered views, such that (Iinp , Itgt ) ∼ V, where V = {I1 , I2 , · · · , IM +N } is a randomly sampled video batch with ordering preserved. Additional temporal operators may also be used to improve the temporal smoothness, such as temporal positional encoding and temporal attention. In contrast with image models, video models thrive at trajectory NVS, but struggle in set NVS. Moreover, all existing video models require both input and target views to be ordered (input views followed by target ones), constraining their input flexibility [10, 19, 22–24].

## 2.3. Remarks and Motivation

Existing tasks pose critical challenges to our design choices. Specifically, our design choices are made to achieve high generation capacity, smooth view interpolation, and flexible input conditioning, as compared in Tab. 1. In this way, we can employ a single model for both tasks, described next.

# 3. Method

We describe our model design and training strategy in Sec. 3.1, then our sampling process at test time in Secs. 3.2 and 3.3. A system overview is provided in Fig. 4.

## 3.1. Model Design and Training

We consider a “M -in N -out” multi-view diffusion model pθ , as notated in Sec. 2.2. We formulate this learning problem as a standard diffusion process [21] without any change.

Architecture. Our model is based on the publicly available SD 2.1 [25], which consists of an auto-encoder and a latent denoising U-Net. Following [8], we inflate the 2D self-attention of each low-resolution residual block into 3D self-attention [26] within the U-Net. To improve model capacity, we add 1D self-attention along the view axis after each self-attention block via skip connection [27, 28], bumping the model parameters from 870M to 1.3B. Optionally, we further tame this model into a video model by introducing 3D convolutions after each residual block via skip connection, similar to [22, 29], yielding 1.5B total parameters. The temporal pathway can be enabled during inference when frames within one forward pass are known to be spatially ordered, enhancing output’s smoothness.

Conditioning. To fine-tune our base model into a multiview diffusion model, we add camera conditioning as Plücker embedding [30] via concatenation [8] and adaptive layer normalization [31]. We normalize π inp and π tgt by first computing the relative pose with respect to the first input camera and then normalizing the scene scale such that all camera positions are within a [−2, 2]3 cube. For each input frame, we first encode its latent then concatenate with its Plücker embedding and a binary mask [8, 22] differentiating between input and target views. For each target frame, we use the noisy state of its latent instead. Additionally, we find it helpful [19] to also inject high-level semantic information via CLIP [32] image embedding. We zero initialize new weights for additional channels in the first layer. In our experiment, we found that our model can quickly adapt to these conditioning changes and produce realistic images with as few as 5K iterations.

Training. Let us define the training context window length T = |Iinp | + |Itgt | = M + N . One natural goal is to support large T such that we can generate a larger set of frames. However, we find that naive training is prone to divergence, and we thus employ a two-stage training curriculum. During the first stage, we train our model with T = 8 with a batch size of 1472 for 100K iterations. In the second stage, we train our model with T = 21 with a batch size of 512 for 600K iterations. Given a training video sequence, we randomly sample the number of input frames M ∈ [1, T − 1] and the frames (Iinp , Itgt ). We find it important to jointly sample I with a smaller subsampling stride to ensure sufficient temporal granularity and avoid missing critical transitions with a small probability (0.2 is used in practice). In the optional video training stage, we only train temporal weights with data sampled with a small subsampling stride and a batch size of 512 for 200K iterations. We shift the signal-to-noise ratio (SNR) in all stages as more noise is necessary to destroy the information when training with more frames, corroborating findings from [8, 33, 34]. The model is trained with squared images with H = W = 576.

## 3.2. Sampling Novel Views

Once the diffusion model is trained, we can sample it for a wide range of NVS tasks during test time. Formally, let us consider a “P -in Q-out” NVS task during testing, where we are given P = |Iinp | input frames and aim to produce Q = |Itgt | target frames. Our goal is to design a generic sampling strategy that works for all P and Q configurations, where P and Q need not be equal to M and N . We make two key observations: First, within a single forward pass, predictions are 3D consistent, provided the model is well-trained. Second, when P + Q > T , Itgt must be split into smaller chunks of size Qi such that P + Qi ⩽ T for the ith forward pass. We term this practice one-pass sampling. However, predictions across these forward passes would be inconsistent unless they share common frames to maintain local consistency within a spatial neighborhood. Building on these observations, we summarize our sampling process under two scenarios: P + Q ⩽ T and P + Q > T .

P + Q ⩽ T . We fit the task within one forward pass for simplicity and consistency. As shown in Appendix D, we find it works better to pad the forward pass to have exactly T frames by repeating the first input image, compared to changing the context window T zero-shot.

P + Q > T . We propose procedural two-pass sampling: In the first pass, we generate anchor frames Iacr using all input frames Iinp . In the second pass, we divide Itgt into chunks and generate them using Iacr (and optionally Iinp ) according to the spatial distribution of Iacr and Itgt . Given the distinct nature of the two tasks of interest—set NVS and trajectory NVS—e.g., differences in the availability of views’ ordering, we design tailored chunking strategies for each task.

For set NVS, we consider nearest procedural sampling. We first generate Iacr based on pre-defined trajectory priors, similar to [8], e.g., 360 trajectories for object-centric scenes, or spiral trajectories for forward-facing scenes. We then divide Itgt into chunks w.r.t. Iacr using nearest neighbor. Specifically, the ith forward pass involves: nearest : {Iacr i } ∪ {Ij | NN(Ij , I ) = Ii }. We considered two strategies of procedural sampling: nearest as described above, and gt + nearest strategy by appending Iinp into each forward pass. We find that the gt + nearest strategy performs better than nearest and thus default to it instead. In the absence of trajectory priors, we revert to one-pass sampling. In practice, employing nearest anchors enhances qualitative consistency, albeit on a limited scale.

For trajectory NVS, we consider interp procedural sampling. We first generate a subset of target frames as Iacr by uniformly spanning the target camera path with a stride ∆ = ⌊ TQ −2 ⌋. We then generate the rest of Itgt as segments between those anchors: interp : {Iacr i , Ii·∆+1 , · · · , I(i+1)·∆−1 , Ii+1 }. Since the input to the model is ordered, we can leverage temporal weights to further improve smoothness (Sec. 4.3). Similarly, gt + interp is possible by appending Iinp with ∆ = ⌊ T −P −2 ⌋. We find that interp is sufficiently robust, and choose it as the default option. The interp strategy drastically outperforms its counterparts (e.g., one-pass, or gt + nearest procedural sampling) in terms of temporal smoothness.

## 3.3. Scaling Sampling for Large P and Q

Next, we examine two special cases when P + Q > T : P > T and Q ≫ T . Here, we make a tailored design for anchor generation in the first pass, while keeping target generation in the second pass unchanged.

P > T . In the semi-dense-view regime (e.g., P = 32), we extend the context window length T zero-shot to accommodate all P input views and anchor views in one pass during anchor generation. Empirically, T can even be extended up to hundreds without severe degradation in photorealism in the generated outputs. We find that the diffusion model generalizes well in this case as long as the input views cover the majority of the scene, shifting the task from generation to primarily interpolation. In the sparse-view regime (i.e., P ≤ 8), we observe similar performance degradation caused by zero-shot extension of T compared to what we have found when P + Q ⩽ T . Refer to Sec. 4.5 for a detailed discussion.

Q ≫ T . When the number of target views Q is large, e.g., in large-set NVS or long-trajectory NVS, even anchors will be chunked into different forwards in the first pass, leading to the inconsistency of anchors. To this end, we maintain a memory bank of anchor views previously generated, as shown in Fig. 5. We generate anchors autoregressively by retrieving their spatially nearest ones from the memory bank, similar to the nearest strategy introduced above for the second pass. In Sec. 4.4, we show that this strategy drastically outperforms the standard practice of reusing temporally nearest anchors previously generated in long video literature [24], in terms of long-range 3D consistency, especially for hard trajectories.

# 4. Experiments

We employ a single model for a spectrum of settings and find that S EVA model generalizes well under the three criteria (Tab. 1). We cover different NVS tasks (set NVS and trajectory NVS) and examine one special task of interest— long trajectory NVS. We also cover different input regimes (single-view, sparse-view, and semi-dense-view). A discussion about several key properties is presented in Sec. 4.5.

## 4.1. Benchmark

Datasets, splits, and the number of input views. We consider (1) object datasets, e.g., OmniObject3D [35] (OO3D) and GSO [36]; (2) object-centric scene datasets, e.g., LLFF [37], DTU [38], CO3D [39], and WildRGBD [40] (WRGBD); and (3) scene datasets, e.g., RealEstate10K [41] (RE10K), Mip-NeRF 360 [42] (Mip360), DL3DV140 [43] (DL3DV), and Tanks and Temples [44] (T&T). We consider a wide range of the number of input views P , ranging from sparse-view regime to semi-dense-view regime, evaluating models’ input flexibility. To establish a comprehensive and rigorous comparison with baselines, we consider different dataset splits utilized in prior works with the same input-view configuration, unless specified as our split (O). These include splits used in 4DiM [12] (D), ViewCrafter [9] (V), pixelSplat [16] (P), ReconFusion [7] (R), SV3D [19] (S), and Long-LRM [18] (L). For example, the 4DiM [12] (D) split on the RE10K dataset is 128 out of all 6711 test scenes with P = 1.

Small-viewpoint versus large-viewpoint NVS. Sweeping across all datasets, splits, and input-view configurations reveals a diverse benchmark of setups. To better evaluate models’ generation capacity and interpolation smoothness (Sec. 2.1), we propose to categorize these setups into two groups—small-viewpont NVS and large-viewpoint NVS— depending on the disparity between Itgt and Iinp . smallviewpoint NVS with smaller disparities emphasizes interpolation smoothness and continuity with nearby input views, whereas large-viewpoint NVS with larger disparities requires a model to generate prominent unseen areas from input observations, predominantly assessing models’ generation capacity. See Tab. 7 for the complete list. Refer to Appendix C for the detailed choice of datasets, splits, the number of input views, and the way to measure disparity.

Baselines. We consider a range of proprietary models, including ReconFusion [7], CAT3D [8], 4DiM [12], LVSM [11], and Long-LRM [18]. We also consider various open-source models, including SV3D [19], MVSplat [17], depthSplat [45], MotionCtrl [46], and ViewCrafter [9]. As outlined in Sec. 2.2, these baselines encompass both regression-based and diffusion-based approaches, providing a comprehensive framework for comparison.

## 4.2. Set NVS

In this section, we focus on comparing our model against prior works, given that set NVS is a task that has been extensively explored.

Quantitative comparison. The input and target views are chosen following splits used in previous methods. The order of target views is not preserved, i.e., Itgt ∼ I. We use standard metrics of peak signal-to-noise ratio (PSNR), learned perceptual image patch similarity [48] (LPIPS), and structural similarity index measure [49] (SSIM). Only PSNR is showcased here due to space limits, with the rest deferred to Appendix D.2. Empirically, our method shows a greater performance improvement on LPIPS, reflecting the photorealism of our results.

For small-viewpoint set NVS, Tab. 2 shows that S EVA sets state-of-the-art results in the majority of splits. In the sparse-view regime (i.e., P ⩽ 8), S EVA excels across different datasets when P > 1. For example, a performance gain of +6.0 dB PSNR is achieved on LLFF with P = 3. In the semi-dense-view regime (e.g., P = 32), S EVA surprisingly performs favorably against the specialized model [18], despite not being specifically designed for this setup. For example, S EVA lags behind the state-of-the-art method [18] by only 1.7 dB on T&T. On object datasets OO3D and GSO, S EVA achieves a significantly higher state-of-the-art PSNR compared to all other methods.

Notably, for small-viewpoint set NVS on the RealEstate10K [41] dataset, S EVA underperforms when in the single-view regime (i.e., P = 1). This issue arises from scale ambiguity in the model due to two factors: (1) it always takes in unit-normalized cameras during training, and (2) it is trained on multiple datasets with diverse scales. This challenge is most pronounced on RE10K, where panning motion dominates. Additionally, the absence of a second input view negates any scale relativity. To address this, for all results with P = 1, we sweep the unit length for camera normalization from 0.1 to 2.0 (with 2.0 used during training), selecting the best scale for each scene. On P split with P = 2, we observe diffusion models lag behind regression-based models that are advantageous in small-viewpoint interpolation. S EVA bridges this gap by improving upon the state-of-the-art diffusion-based model by +4.2 dB. On R split with P = 3, the advantage of S EVA is pronounced exceeding the previously best result by +1.9 dB. Notably, ViewCrafter excels on V split due to capacity taking in wide-aspect-ratio images and thus more input pixels than others with square images. The advantage of ViewCrafter on V split diminishes on CO3D since the majority of informative pixels are centrally located.

For large-viewpoint set NVS, Tab. 3 shows that S EVA’s quantitative advantages are even more prominent here, revealing clear benefits of S EVA in terms of generation capacity when the camera spans a large spatial range. On Mip360 with P = 3, S EVA improves over previous state-of-the-art method CAT3D [8] by +0.6 dB PSNR. On harder scenes like DL3DV and T&T with different input-view configurations, S EVA obtains a clear performance lead. On OO3D and GSO with P = 1, although the performances of S EVA and previous state-of-the-art method [19] are similar, we qualitatively observe more photorealistic and sharper output from our model.

Qualitative comparison. Fig. 6 top panel shows a qualitative comparison with diverse baselines. For smallviewpoint set NVS, the output from S EVA with the best scale exhibits desirable alignment with the ground truth while being more photorealistic in details. Compared with LVSM [11] on the P split of RE10K, S EVA produces sharper images, also corroborating that lower PSNR arises from scale ambiguity rather than interpolation quality. Similar trends hold when compared to Long-LRM [18] on DL3DV with P = 32. For large-viewpoint set NVS, we compare with DepthSplat [45] on DL3DV with P = 3. DepthSplat fails to produce reasonable results when the viewpoint change is too large and falls short in overall visual quality.

Comparison of 3D reconstruction. To enable a direct quantitative comparison with prior works [7, 8], we adopt the few-view 3D reconstruction pipeline described in [8]. For each scene, we first generate 8 videos conditioned on the same input views following different camera paths, summing into 720 generated views. Then, both the input views and generated views are distilled into a 3DGSMCMC [50] representation without point cloud initialization. We optimize the camera parameters and apply LPIPS loss [51] during the distillation. Finally, we render the distilled 3D model on the test views and report the performance in Tab. 4. S EVA shows a consistent performance lead.

## 4.3. Trajectory NVS

In this section, we focus on qualitative demonstration, given that trajectory NVS is an underexplored task. We then compare against prior arts both qualitatively and quantitatively.

Qualitative results. Fig. 2 presents qualitative results, illustrating trajectories of varying complexities with different numbers of input views across diverse types, including object-centric scene-level, scene-level, real-world, and textprompted from image diffusion models [25], etc.

In the single-view regime (i.e., P = 1), we manually craft a set of common camera movements/effects, e.g., lookat 360, spiral, panning, zoom-in, zoom-out, dolly zoom, etc. We observe that S EVA generalizes to a wide range of images and demonstrates accurate camera-following capacity. Excitingly, our model derives reasonable output with a dolly zoom effect (the second row of Fig. 2). In the FERN scene from the third row of Fig. 2, our model demonstrates its ability to generate plausible outputs even when moving close to or passing through an object—despite never being explicitly trained for such scenarios. This highlights the expressiveness of our model. An extensive sweeping of camera movements on 4 types of images is provided in Figs. 15 to 18.

In the sparse-view regime with few input views (i.e., 1 < P ⩽ 8), we observe that S EVA demonstrates strong generalization to in-the-wild real-world images and versatility in adapting to different numbers of input views. The output forms a smooth trajectory video with subtle temporal flickering, revealing its capacity to interpolate between views smoothly. In the last row of Fig. 2, our model generates plausible results at the end of the trajectory—an area unseen in the input observations—demonstrating its strong generation capacity. In the semi-dense-view regime (i.e., P > 9), we similarly find that S EVA is surprisingly able to produce a smooth trajectory video with minimal artifacts. Please check the website for video results.

Qualitative comparison. Fig. 6 bottom panel presents a qualitative comparison with diverse baselines. In the single view regime (i.e., P = 1), we compare to 4DiM [12] and CAT3D [8]. We observe more photo-realistic and sharper output from our model, especially in the background area for object-centric scenes. 4DiM outputs tend to be cartoonish and over-simplistic, given that the model is only trained on RE10K. In the sparse-view regime with few input views (i.e., P = 3), we compare with CAT3D and observe that our model demonstrates more photo-realistic textures, especially in the background. For start-end-view interpolation considered in ViewCrafter [9] with P = 2, our model produces smooth transitions across trajectories, although it exhibits slight flickering between adjacent frames, particularly in regions with significant high-frequency detail.

Quantitative comparison. We use the same input views as in the set NVS for each split. We use all frames from each scene as target views such that they form a smoothly transitioning trajectory video, i.e., Itgt ∼ V. We use PSNR as metrics and compare with baselines in Tab. 5.

For small-viewpoint trajectory NVS, Tab. 5 compares S EVA with baselines on PSNR. S EVA performs favorably against other methods in V split with P = 1. The performance lead of ViewCrafter is mainly attributed to its training on high-resolution images. For large-viewpoint trajectory NVS with P = 3, S EVA consistency sets new state-of-the-art results. Applying the temporal pathway further boosts performance and improves smoothness, indicating the benefits of the gated architecture.

Ablation on two-pass procedural sampling. We conduct an ablation study comparing the default interp procedural sampling with one-pass sampling and alternative procedural sampling strategies.

Quantitatively, beyond evaluating PSNR on individual views, we assess 3D consistency using the PSNR on 3D renderings of that same view and SED [12, 53] score. To compute the SED score, we first apply SIFT [54] to detect keypoints in two images. For each keypoint in the first image, we determine its corresponding epipolar line in the second image and measure the shortest distance to its match. Additionally, we report Motion Smoothness (MS) from VBench [52], a benchmark designed to evaluate temporal coherence in video generative models. As shown in Tab. 6, interp procedural sampling demonstrates a clear advantage over its alternatives, with the integration of the temporal pathway further reinforcing its superiority.

Qualitative comparisons in Fig. 7 show that one-pass sampling introduces visible temporal flickering and abrupt visual changes. In contrast, interp produces the smoothest transitions, outperforming gt + nearest and mitigating noticeable flickering.

## 4.4. Long-Trajectory NVS

Fig. 8 presents a qualitative demonstration of NVS over a long trajectory of up to 1000 frames. As the camera orbits the TELEPHONE BOOTH for multiple rounds, the generated views in each round from similar viewpoints can be drastically different since they are far away from each other temporally. With the memory bank maintaining previously generated anchors, S EVA achieves robust 3D consistency for long-trajectory NVS, e.g., the building in front of and the plantation after the booth. Comparing it to using temporal nearest anchors previously generated, using spatially nearest ones demonstrates a clear advantage. The memory mechanism has been concurrently explored in previous works [9, 55], leveraging explicit intermediate 3D representations such as dense point clouds predicted by DUSt3R [56]. In contrast, our model demonstrates greater robustness and generalizability to in-the-wild data, as it is not constrained by the quality of DUSt3R’s output, which often becomes unreliable in quality for data outside of its training domain, e.g., text-prompted images.

## 4.5. Discussions

Zero-shot generalization of context window length T . We surprisingly find our model, though only trained on T = 21 frames, can generalize reasonably to larger T during sampling in the semi-dense-view regime. On our split of T&T for set NVS, we evaluate the predictions against ground truth in both sparse-view (i.e., 1 ≤ P ≤ 8) and semi-dense-view regime (i.e., 9 ≤ P ) using PSNR↑ and Image Quality↑ [52]. Image Quality refers to the distortion (e.g., over-exposure, noise, blur) presented in the generated image. We experiment with different sampling strategies: one-pass sampling zero-shot extending the context window length T ; two-pass procedural sampling by first generating anchor views using nearest−K (K < T ) input views and then interpolating anchor views into target views.

Our results are shown in Fig. 9. Procedural sampling with the nearest−K anchor views plateau after taking K views as input, indicating inefficiencies in procedural sampling and an inability to effectively utilize all available input views when P > T . Conversely, the metrics steadily improve with respect to the number of input frames for onepass sampling with T extending to P + Q in a zero-shot manner. However, we observe that this generalization fails in the sparse-view regime, resulting in blurry samples, as indicated by the low Image Quality when P < 9 and qualitative samples when P = 3. In the semi-dense-view setting, although quantitative metrics show minimal differences between one-pass and procedural sampling, we consistently observe that one-pass produces more 3D-consistent samples, as illustrated in the bottom-right figure.

Zero-shot generalization of image resolution. Surprisingly, we find our model, despite being trained only on square images with H = W = 576, generalizes well to different image resolution during sampling, similar to [57]. As shown in Fig. 10, S EVA can produce high-quality results in both portrait (16 : 9) and landscape (9 : 16) orientations of different image resolutions.

Guidance scale on generation uncertainty. We employ classifier-free guidance [58] (CFG) to enhance sampling quality. Empirically, we find that the CFG scale, a hyperparameter at test time, has a significant impact on the final result [22], as shown in Fig. 11. Specifically, the optimal CFG scale is strongly correlated with the inherent uncertainty of the generation. When uncertainty is high (top row), a higher CFG scale (e.g., 5) is preferable to prevent excessive blurriness in the generated samples. Conversely, when uncertainty is low (bottom row), a lower CFG scale (e.g., 3) helps avoid oversaturation. In practice, setting the CFG scale between 2 and 5 consistently produces high-quality results across all our samples.

Sampling diversity of unseen areas. Fig. 12 demonstrates the capability of the model to generate diverse and plausible predictions for unseen regions of input observations. In the first row, the input view depicts a frontal view of a classical statue. We sample multiple back views by varying the random seeds, producing distinct yet coherent interpretations of the unseen geometry and texture while preserving fidelity to the input. Similarly, in the second row, the model generates multiple plausible continuations of the scene given an input view of a scenic road, each reflecting unique variations in environmental and structural details. These results highlight the model’s ability to synthesize realistic and diverse outputs for occluded or ambiguous regions.

# 5. Conclusion

We present S TABLE V IRTUAL C AMERA (S EVA), a generalist diffusion model for novel view synthesis that balances large viewpoint changes and smooth interpolation while supporting flexible input and target configurations. By designing a diffusion-based architecture without 3D representation, a structured training strategy, and a two-pass procedural sampling approach, S EVA achieves 3D consistent rendering across diverse NVS tasks. Extensive benchmarking demonstrates its superiority over existing methods, with strong generalization to real-world scenes. For broader impact and limitations, please refer to the appendix.

# References

[1] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, pages 405–421, 2020.

[2] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 2022.

[3] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023.

[4] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In CVPR, 2021.

[5] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3D using 2D diffusion. arXiv, 2022.

[6] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3D object. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.

[7] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P Srinivasan, Dor Verbin, Jonathan T Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21551–21561, 2024.

[8] Ruiqi Gao*, Aleksander Holynski*, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole*. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv, 2024.

[9] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. ViewCrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024.

[10] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 1–11, 2024.

[11] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: A large view synthesis model with minimal 3d inductive bias, 2024.

[12] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David J Fleet. Controlling space and time with diffusion models. arXiv preprint arXiv:2407.07860, 2024.

[13] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from a single real image. arXiv preprint arXiv:2310.17994, 2023.

[14] Virtual camera system. https://en.wikipedia. org/wiki/Virtual_camera_system. Accessed: 5-Mar-2025.

[15] How weta digital’s virtual camera transforms cinematography. https://www.youtube.com/watch?v=IctZRxLEN4. Accessed: 5-Mar-2025.

[16] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In arXiv, 2023.

[17] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370–386. Springer, 2025.

[18] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Longsequence large reconstruction model for wide-coverage gaussian splats. arXiv preprint arXiv:2410.12781, 2024.

[19] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitrii Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. SV3D: Novel multi-view synthesis and 3D generation from a single image using latent video diffusion. In European Conference on Computer Vision, 2024.

[20] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021.

[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.

[22] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.

[23] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024.

[24] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv, 2022.

[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684–10695, June 2022.

[26] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023.

[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[28] P Goyal. Accurate, large minibatch sg d: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

[29] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563–22575, 2023.

[30] Julius Plucker. Xvii. on a new geometry of space. Philosophical Transactions of the Royal Society of London, (155):725– 791, 1865.

[31] Chuanxia Zheng and Andrea Vedaldi. Free3D: Consistent novel view synthesis without 3D representation. arXiv preprint arXiv:2312.04551, 2023.

[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.

[33] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. arXiv preprint arXiv:2301.11093, 2023.

[34] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICLR, 2024.

[35] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3D: Large-vocabulary 3D object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 803–814, 2023.

[36] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google Scanned Objects: A highquality dataset of 3D scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 2553–2560. IEEE, 2022.

[37] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (ToG), 38(4):1–14, 2019.

[38] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 406–413, 2014.

[39] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In International Conference on Computer Vision, 2021.

[40] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos, 2024.

[41] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018.

[42] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5855– 5864, 2021.

[43] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: A large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22160–22169, 2024.

[44] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):1–13, 2017.

[45] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. arXiv preprint arXiv:2410.13862, 2024.

[46] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, TsungYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023.

[47] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19697–19705, 2023.

[48] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586–595, 2018.

[49] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004.

[50] Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. 3d gaussian splatting as markov chain monte carlo. arXiv preprint arXiv:2404.09591, 2024.

[51] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric, 2018.

[52] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.

[53] Jason J Yu, Fereshteh Forghani, Konstantinos G Derpanis, and Marcus A Brubaker. Long-term photometric consistent novel view synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7094–7104, 2023.

[54] Huiyu Zhou, Yuan Yuan, and Chunmei Shi. Object tracking using sift features and mean shift. Computer vision and image understanding, 113(3):345–352, 2009.

[55] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. 2024.

[56] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024.

[57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.

[58] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. arXiv:2207.12598, 2022.

[59] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-NeRF for shape-guided generation of 3D shapes and textures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12663–12673, 2023.

[60] Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. arXiv:1907.05600, 2019.

[61] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Light field neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8269–8279, 2022.

[62] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10208– 10217, 2024.

[63] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3D. arXiv preprint arXiv:2311.04400, 2023.

[64] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models, 2022.

[65] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3D object. International Conference on Computer Vision, 2023.

[66] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023.

[67] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. SyncDreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453, 2023.

[68] Antoine Mercier, Ramin Nakhli, Mahesh Reddy, Rajeev Yasarla, Hong Cai, Fatih Porikli, and Guillaume Berger. HexaGen3D: Stablediffusion is just one step away from fast and diverse Text-to-3D generation. arXiv preprint arXiv:2401.07727, 2024.

[69] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. arXiv preprint arXiv:2311.17984, 2023.

[70] Petar Veličković, Christos Perivolaropoulos, Federico Barbero, and Razvan Pascanu. softmax is not enough (for sharp out-of-distribution). arXiv preprint arXiv:2410.01104, 2024.

[71] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion, 2025.

[72] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion, 2024.

[73] Stability AI. Stable diffusion 3.5. https://stability. ai/news/introducing- stable- diffusion- 35, 2024.