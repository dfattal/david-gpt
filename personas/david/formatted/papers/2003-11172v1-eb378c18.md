---
title: "Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset"
docType: paper
url: https://arxiv.org/abs/2003.11172v1
scraped_at: "2025-09-26T19:51:47.715Z"
word_count: 5567
extraction_quality: high
authors:
  - name: Yiwen Hua
    affiliation: Leia Inc.
  - name: Puneet Kohli
    affiliation: Leia Inc.
  - name: Pritish Uplavikar
    affiliation: Leia Inc.
  - name: Anand Ravi
    affiliation: Leia Inc.
  - name: Saravana Gunaseelan
    affiliation: Leia Inc.
  - name: Jason Orozco
    affiliation: Leia Inc.
  - name: Edward Li
    affiliation: Leia Inc.
venue: "arXiv"
publicationYear: 2020
abstract: "With the mass-market adoption of dual-camera mobile phones, leveraging stereo information in computer vision has become increasingly important. Current state-of-the-art methods utilize learning-based algorithms, where the amount and quality of training samples heavily influence results. Existing stereo image datasets are limited either in size or subject variety. Hence, algorithms trained on such datasets do not generalize well to scenarios encountered in mobile photography. We present Holopix50k, a novel in-the-wild stereo image dataset, comprising 49,368 image pairs contributed by users of the Holopix™ mobile social platform. In this work, we describe our data collection process and statistically compare our dataset to other popular stereo datasets. We experimentally show that using our dataset significantly improves results for tasks such as stereo super-resolution and self-supervised monocular depth estimation. Finally, we showcase practical applications of our dataset to motivate novel works and use cases. The dataset is available at http://github.com/leiainc/holopix50k."
keywords: ["Stereo vision", "stereo image dataset", "3D photography", "stereo super-resolution", "disparity estimation"]
technologies: ["lightfield", "autostereoscopic displays", "holography", "computer vision"]
---

# Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset

## 1. Introduction

Most mobile phones now come with two or more cameras, enabling consumer applications such as artificial depth of field (portrait mode), AI-based photo optimization, and 3D photography [29]. Through dual-cameras, mobile phones can perceive depth and lighting of a scene using complementary information from the stereo image pairs. Various applications utilize stereo imagery such as Holopix to produce immersive 3D Photography experiences. More recently, larger platforms such as Facebook and Snap Inc. have also demonstrated unique and exciting user experiences enabled via stereo imagery.

Advancements in self-driving cars and robotics technologies with multi-camera configurations have accelerated research and development of computer vision algorithms [19][56], many of which utilize additional stereo information [40][17][30]. The mass-market adoption of dual-cameras in mobile phones has led to an increased need for these algorithms to work in casual in-the-wild scenarios encountered during mobile photography. A majority of state-of-the-art methods in this domain are based on deep learning methods, where accuracy and quality scale with the amount of data available for training. Current datasets either cover a limited subset of real-world scenarios [16][8] or are taken in a laboratory setting [48][9]. Further, there is an increased need for a large-scale stereo dataset representative of the diversity of real-world scenarios to enable generalization. Though recent works have been able to collect stereo pairs [55][57] from various internet platforms, they may not be diverse or extensive enough to represent in-the-wild scenarios captured from mobile devices.

In this work, we present a new large-scale stereo image dataset that is completely user-generated and captured mostly from mobile phone cameras in-the-wild. In our novel Holopix50k dataset, we present 49,368 (roughly 50k) rectified stereo image pairs collected from the mobile social media platform Holopix. This is larger than comparable stereo image datasets by an order of magnitude. To our knowledge, this is the first large-scale stereo image dataset collected from a mobile-based social platform where users curate and upload stereo images.

## 2. Related Work

### 2.1 Stereo Datasets

**Real-world Datasets.** There are a variety of real-world stereo datasets currently available. Some of the more popular ones, such as KITTI [16][38], Middlebury [48], and the NYU Indoor [41] datasets, all provide ground truth depth or disparities for the stereo scenes. However, these datasets either have a limited number of images, or the scenes are for a specific domain. For example, the KITTI datasets were mainly developed for self-driving vehicle use-cases, while the few scenes in Middlebury are all captured in a laboratory setting. Other real-world stereo datasets include Make3D [47], ETH3D [49], CMLA [9], and Cityscapes [8] — each focused on a specific domain. More recent datasets such as Flickr1024 [57] and WSVD [55] provide more diverse scenes, however, Flickr1024 is relatively small compared to our dataset and WSVD images score significantly lower on quality metrics.

**Synthetic Datasets.** The challenge with real-world stereo datasets is collecting ground truth information at-scale that covers a large variety of complex scene types, lighting effects, and motion scenarios. It is also difficult to collect data with different camera baselines and depth ranges. Recent works introduce synthetic datasets that take advantage of the rise of efficient, high-quality rendering techniques. Examples include MPI Sintel [4], SceneFlow [35], UnrealStereo [61], and the 3D Ken Burns [42] datasets. The challenge with such datasets is that techniques based on these datasets still suffer from the domain shift problem [46] when adapting to real-world scenarios.

### 2.2 Stereo Image Tasks

**Stereo Disparity Estimation.** Traditional stereo disparity estimation algorithms rely on block-matching methods [22]. More recently, there has been a significant interest in learning-based techniques due to advances in neural networks [60][59][36][27].

**Monocular Depth Estimation.** Monocular depth estimation aims to estimate the depth of a scene from a single image [18][12]. This is an ill-posed problem as image textures do not directly correspond to depth [2].

**Stereo Super-Resolution.** Super-resolution is a well-known task in computer vision that aims to construct a high-resolution image from their low-resolution versions [7][11][13]. Stereo super-resolution techniques aim to incorporate a second low-resolution image to increase the quality of the super-resolved high-resolution image.

## 3. The Holopix50k Dataset

Holopix50k is the largest in-the-wild stereo image dataset crowd-sourced from a social media platform to date, containing 49,368 stereo pairs (98,736 images in total).

### 3.1 Data Acquisition and Processing

The Holopix platform is the only major social media platform built specifically for sharing 3D Photography where multiple viewpoints of a scene are captured and viewed with a parallax effect [29].

**Data Collection.** We initially collect about 70k stereo image pairs from the Holopix platform that were suitable for use in the dataset. In the case of mismatching resolution in the left and right stereo images, we downscale the larger of the two images to match the resolution of the smaller image for consistency.

**Removing Vertical Disparity.** As our dataset originates from cameras that are horizontally aligned, the stereo pairs are expected only to have horizontal disparity. After filtering out stereo pairs with vertical disparity, we are left with roughly 60k images in our dataset.

**Disparity-based Filtering.** We filter out images by analyzing the disparity maps produced by our stereo disparity network and removing the images that produce extreme disparity failures. Our filtering process removes up to 95% of image pairs with poor stereo characteristics, after which, our final dataset includes 49,368 stereo pairs.

### 3.2 Dataset Exploration

As an exploration of our dataset, we run the Mask-RCNN [21] object detector on the left image of each stereo pair. The detector we use is pre-trained on the MS COCO dataset [33], containing 91 object categories for common objects. We can see that there is a good mixture of common objects including people, animal species, plants, vehicles, furniture, electronics, and food types.

## 4. Comparison with Existing Datasets

We show the results of statistical comparison of the Holopix50k dataset to other popular stereo datasets. Table 1 shows the results of comparison against KITTI2012 [16], KITTI2015 [38], Middlebury [48], ETH3D [49], Flickr1024 [57], and Web Stereo Video (WSVD) [55] datasets. We compare the amount of information present in the datasets using the entropy measure [54], and use BRISQUE [39], SR-metric [34], and ENIQA [6], in order to assess the no-reference perceptual quality of the datasets.

## 5. Experiments with Holopix50k

### 5.1 Stereo Super-Resolution

We demonstrate how the Holopix50k dataset helps boost the performance on the current state-of-the-art method, namely PASSRNet [56], for stereo super-resolution. The PASSRNet model is originally trained on the Flickr1024 [57] training dataset. We re-train it for the 4x super-resolution task in a variety of experiments using the training code provided by the authors.

### 5.2 Self-supervised Monocular Depth Estimation

We use the Holopix50k dataset to fine-tune a self-supervised monocular depth estimation model, namely Monodepth2 [19]. The model is originally trained on the KITTI dataset [16] and predicts a dense depth map from a single image.

### 5.3 Disparity Models Trained using Holopix50k

**Stereo Disparity Estimation.** We use the Holopix50k dataset to train a stereo disparity estimation network for mobile inference. Our network follows a U-Net like architecture [45] and has a computation footprint of ~340k parameters (~1.5 GFLOPS).

**Real-time Disparity Estimation.** We follow the same training pipeline and methodology as our stereo disparity estimation network to train a real-time disparity estimation network. This network is optimized to have a computation footprint of ~15k parameters (~0.15 GFLOPS).

**Monocular Depth Estimation.** We also train a monocular depth estimation network that is semi-supervised using a combination of the Holopix50k and Megadepth datasets [32]. We model this as a generative image translation task conditioned by stereo depth information using an architecture similar to Pix2pix [24] with the PatchGAN discriminator [24].

## 6. Future Work

The Holopix50k dataset is our first release of a large-scale dataset scraped from the Holopix platform. As the platform continues to grow and increase its user-base, we plan to continue supporting the dataset with multiple future iterations. We also plan to share more details of the models trained using the Holopix50k dataset.

## 7. Conclusion

In this work, we present Holopix50k, a large-scale in-the-wild stereo image dataset that contains a variety of diverse scene types and semantic image content. To the best of our knowledge, this is the largest publicly released stereo image dataset collected from a mobile social media application. We showcase the superiority of our dataset, as a whole as well as its subsets, compared to other stereo datasets. We also demonstrate how using our dataset improves the performance of stereo image tasks, such as stereo super-resolution and self-supervised monocular depth estimation.

## References

1. Beier, T., Neely, S.: Feature-based image metamorphosis. In: Proceedings of the 19th Annual Conference on Computer Graphics and Interactive Techniques. p. 3542. SIGGRAPH 92, Association for Computing Machinery, New York, NY, USA (1992).
2. Bhoi, A.: Monocular depth estimation: A survey. CoRR abs/1901.09402 (2019).
3. de Boor, C.: Bicubic spline interpolation. Journal of Mathematics and Physics 41(1-4), 212-218 (1962).
4. Butler, D., Wulff, J., Stanley, G., Black, M.: Mpi-sintel optical flow benchmark: Supplemental material. In: MPI-IS-TR-006, MPI for Intelligent Systems (2012).
5. Chen, D., Yuan, L., Liao, J., Yu, N., Hua, G.: Stereoscopic neural style transfer. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6654-6663 (June 2018).
... (omitted for brevity)