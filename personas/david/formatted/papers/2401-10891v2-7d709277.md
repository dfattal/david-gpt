---
title: 'Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data'
docType: paper
url: https://arxiv.org/abs/2401.10891v2
scraped_at: 2025-09-26T19:53:13.864Z
word_count: 3168
extraction_quality: high
authors:
  - name: Lihe Yang
    affiliation: HKU
  - name: Bingyi Kang
    affiliation: TikTok
  - name: Zilong Huang
    affiliation: TikTok
  - name: Xiaogang Xu
    affiliation: CUHK
  - name: Jiashi Feng
    affiliation: TikTok
  - name: Hengshuang Zhao
    affiliation: HKU
venue: arXiv
publicationYear: 2024
abstract: This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet.
keywords: ["monocular depth estimation", "foundation model", "unlabeled data", "self-training", "semantic segmentation", "zero-shot"]
technologies: ["lightfield", "autostereoscopic displays", "holography", "computer vision"]
---

# Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data

## 1. Introduction

The field of computer vision and natural language processing is currently experiencing a revolution with the emergence of "foundation models" that demonstrate strong zero-/few-shot performance in various downstream scenarios. These successes primarily rely on large-scale training data that can effectively cover the data distribution. Monocular Depth Estimation (MDE), which is a fundamental problem with broad applications in robotics, autonomous driving, virtual reality, etc., also requires a foundation model to estimate depth information from a single image. However, this has been underexplored due to the difficulty of building datasets with tens of millions of depth labels. MiDaS made a pioneering study along this direction by training an MDE model on a collection of mixed labeled datasets. Despite demonstrating a certain level of zero-shot ability, MiDaS is limited by its data coverage, thus suffering disastrous performance in some scenarios.

In this work, our goal is to build a foundation model for MDE capable of producing high-quality depth information for any images under any circumstances. We approach this target from the perspective of dataset scaling-up. Traditionally, depth datasets are created mainly by acquiring depth data from sensors, stereo matching, or SfM, which is costly, time-consuming, or even intractable in particular situations. We instead, for the first time, pay attention to large-scale unlabeled data. Compared with stereo images or labeled images from depth sensors, our used monocular unlabeled images exhibit three advantages: (i) (simple and cheap to acquire) Monocular images exist almost everywhere, thus they are easy to collect, without requiring specialized devices. (ii) (diverse) Monocular images can cover a broader range of scenes, which are critical to the model generalization ability and scalability. (iii) (easy to annotate) We can simply use a pre-trained MDE model to assign depth labels for unlabeled images, which only takes a feedforward step. More than efficient, this also produces denser depth maps than LiDAR and omits the computationally intensive stereo matching process.

We design a data engine to automatically generate depth annotations for unlabeled images, enabling data scaling-up to arbitrary scale. It collects 62M diverse and informative images from eight public large-scale datasets, e.g., SA-1B, Open Images, and BDD100K. We use their raw unlabeled images without any forms of labels. Then, in order to provide a reliable annotation tool for our unlabeled images, we collect 1.5M labeled images from six public datasets to train an initial MDE model. The unlabeled images are then automatically annotated and jointly learned with labeled images in a self-training manner.

## 2. Related Work

### Monocular depth estimation (MDE)
Early works primarily relied on handcrafted features and traditional computer vision techniques. They were limited by their reliance on explicit depth cues and struggled to handle complex scenes with occlusions and textureless regions. Deep learning-based methods have revolutionized monocular depth estimation by effectively learning depth representations from delicately annotated datasets. Eigen et al. first proposed a multi-scale fusion network to regress the depth. Following this, many works consistently improve the depth estimation accuracy by carefully designing the regression task as a classification task, introducing more priors, and better objective functions. Despite the promising performance, they are hard to generalize to unseen domains.

### Zero-shot depth estimation
Our work belongs to this research line. We aim to train an MDE model with a diverse training set and thus can predict the depth for any given image. Some pioneering works explored this direction by collecting more training images, but their supervision is very sparse and is only enforced on limited pairs of points. To enable effective multi-dataset joint training, a milestone work MiDaS utilizes an affine-invariant loss to ignore the potentially different depth scales and shifts across varying datasets. Thus, MiDaS provides relative depth information. Recently, some works take a step further to estimate the metric depth. However, in our practice, we observe such methods exhibit poorer generalization ability than MiDaS, especially its latest version. Besides, as demonstrated by ZoeDepth, a strong relative depth estimation model can also work well in generalizable metric depth estimation by fine-tuning with metric depth information. Therefore, we still follow MiDaS in relative depth estimation, but further strengthen it by highlighting the value of large-scale monocular unlabeled images.

### Leveraging unlabeled data
This belongs to the research area of semi-supervised learning, which is popular with various applications. However, existing works typically assume only limited images are available. They rarely consider the challenging but realistic scenario where there are already sufficient labeled images but also larger-scale unlabeled images. We take this challenging direction for zero-shot MDE. We demonstrate that unlabeled images can significantly enhance the data coverage and thus improve model generalization and robustness.

## 3. Methods (Depth Anything)

Our work utilizes both labeled and unlabeled images to facilitate better monocular depth estimation (MDE). Formally, the labeled and unlabeled sets are denoted as D¹ = {(xi, di)} and Du = {ui} respectively. We aim to learn a teacher model T from D¹. Then, we utilize T to assign pseudo depth labels for Du. Finally, we train a student model S on the combination of labeled set and pseudo labeled set.

### 3.1. Learning Labeled Images

This process is similar to the training of MiDaS. However, since MiDaS did not release its code, we first reproduced it. Concretely, the depth value is first transformed into the disparity space by d = 1/t and then normalized to 0~1 on each depth map. To enable multi-dataset joint training, we adopt the affine-invariant loss to ignore the unknown scale and shift of each sample. To obtain a robust monocular depth estimation model, we collect 1.5M labeled images from 6 public datasets. Furthermore, to strengthen the teacher model T learned from these labeled images, we adopt the DINOV2 pre-trained weights to initialize our encoder. In practice, we apply a pre-trained semantic segmentation model to detect the sky region, and set its disparity value as 0 (farthest).

### 3.2. Unleashing the Power of Unlabeled Images

This is the main point of our work. Distinguished from prior works that laboriously construct diverse labeled datasets, we highlight the value of unlabeled images in enhancing the data coverage. Nowadays, we can practically build a diverse and large-scale unlabeled set from the Internet or public datasets of various tasks. Also, we can effortlessly obtain the dense depth map of monocular unlabeled images simply by forwarding them to a pre-trained well-performed MDE model. This is much more convenient and efficient than performing stereo matching or SfM reconstruction for stereo images or videos. We select eight large-scale public datasets as our unlabeled sources for their diverse scenes. They contain more than 62M images in total.

Technically, given the previously obtained MDE teacher model T, we make predictions on the unlabeled set Du to obtain a pseudo labeled set Du. With the combination set D¹ U Du of labeled images and pseudo labeled images, we train a student model S on it. To address the dilemma of limited knowledge gain, we propose to challenge the student with a more difficult optimization target for additional visual knowledge on unlabeled images. We inject strong perturbations to unlabeled images during training. It compels our student model to actively seek extra visual knowledge and acquire invariant representations from these unlabeled images.

### 3.3. Semantic-Assisted Perception

There exist some works improving depth estimation with an auxiliary semantic segmentation task. We believe that arming our depth estimation model with such high-level semantic-related information is beneficial. Therefore, we aim to seek more informative semantic signals to serve as auxiliary supervision for our depth estimation task. We are greatly astonished by the strong performance of DINOv2 models in semantic-related tasks, e.g., image retrieval and semantic segmentation, even with frozen weights without any fine-tuning. Motivated by these clues, we propose to transfer its strong semantic capability to our depth model with an auxiliary feature alignment loss. The feature space is high-dimensional and continuous, thus containing richer semantic information than discrete masks.

## 4. Results (Experiment)

### 4.1. Implementation Details

We adopt the DINOv2 encoder for feature extraction. Following MiDaS, we use the DPT decoder for depth regression. All labeled datasets are simply combined together without re-sampling. In the first stage, we train a teacher model on labeled images for 20 epochs. In the second stage of joint training, we train a student model to sweep across all unlabeled images for one time. The unlabeled images are annotated by a best-performed teacher model with a ViT-L encoder. The ratio of labeled and unlabeled images is set as 1:2 in each batch.

### 4.2. Zero-Shot Relative Depth Estimation

As aforementioned, this work aims to provide accurate depth estimation for any image. Therefore, we comprehensively validate the zero-shot depth estimation capability of our Depth Anything model on six representative unseen datasets: KITTI, NYUv2, Sintel, DDAD, ETH3D, and DIODE. We compare with the best DPT-BEITL-512 model from the latest MiDaS v3.1, which uses more labeled images than us. As shown in the paper's tables, both with a ViT-L encoder, our Depth Anything surpasses the strongest MiDaS model tremendously across extensive scenes in terms of both the AbsRel (absolute relative error) and δ₁ metrics.

### 4.3. Fine-tuned to Metric Depth Estimation

Apart from the impressive performance in zero-shot relative depth estimation, we further examine our Depth Anything model as a promising weight initialization for downstream metric depth estimation. We initialize the encoder of downstream MDE models with our pre-trained encoder parameters and leave the decoder randomly initialized. The model is fine-tuned with corresponding metric depth information. We examine two representative scenarios: 1) in-domain metric depth estimation, and 2) zero-shot metric depth estimation. Our model outperforms previous best methods remarkably.

### 4.4. Fine-tuned to Semantic Segmentation

In our method, we design our MDE model to inherit the rich semantic priors from a pre-trained encoder via a simple feature alignment constraint. Here, we examine the semantic capability of our MDE encoder. Specifically, we fine-tune our MDE encoder to downstream semantic segmentation datasets. Our encoder from large-scale MDE training is superior to existing encoders from large-scale ImageNet-21K pre-training.

### 4.5. Ablation Studies

Ablation studies were conducted to validate the effectiveness of challenging the student model with strong perturbations when learning unlabeled images and the semantic constraint. The results show that simply adding unlabeled images with pseudo labels does not necessarily bring gains, but with strong perturbations, the model generalization ability is significantly enhanced. The semantic constraint further amplifies this power.

### 4.6. Qualitative Results

The model's predictions were visualized on six unseen datasets, showing robustness to test images from various domains. When compared with MiDaS, the proposed model produces more accurate depth estimation.

## 5. Conclusion

In this work, we present Depth Anything, a highly practical solution to robust monocular depth estimation. Different from prior arts, we especially highlight the value of cheap and diverse unlabeled images. We design two simple yet highly effective strategies to fully exploit their value: 1) posing a more challenging optimization target when learning unlabeled images, and 2) preserving rich semantic priors from pre-trained models. As a result, our Depth Anything model exhibits excellent zero-shot depth estimation ability, and also serves as a promising initialization for downstream metric depth estimation and semantic segmentation tasks.

## 6. References

[1] Manuel López Antequera, Pau Gargallo, Markus Hofinger, Samuel Rota Bulò, Yubin Kuang, and Peter Kontschieder. Mapillary planet-scale depth dataset. In ECCV, 2020.
[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022.
[3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In CVPR, 2021.
[4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv:2302.12288, 2023.
[5] Reiner Birkl, Diana Wofk, and Matthias Müller. Midas v3.1-a model zoo for robust monocular relative depth estimation. arXiv:2307.14460, 2023.
... (references continue in original document)