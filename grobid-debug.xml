<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAT3D: Create Anything in 3D with Multi-View Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-16">16 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aleksander</forename><surname>Hoły Ński</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Brussee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
						</author>
						<title level="a" type="main">CAT3D: Create Anything in 3D with Multi-View Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-16">16 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">BDD3B5D27EFC8CAA45141F3775EE1FA5</idno>
					<idno type="arXiv">arXiv:2405.10314v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-09-19T03:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: CAT3D enables 3D scene creation from any number of generated or real images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The demand for 3D content is higher than ever, since it is essential for enabling real-time interactivity for games, visual effects, and wearable mixed reality devices. Despite the high demand, high-quality 3D content remains relatively scarce. Unlike 2D images and videos which can be easily captured with consumer photography devices, creating 3D content requires complex specialized tools and a substantial investment of time and effort.</p><p>Fortunately, recent advancements in photogrammetry techniques have greatly improved the accessibility of 3D asset creation from 2D images. Methods such as NeRF <ref type="bibr" target="#b0">[1]</ref>, Instant-NGP <ref type="bibr" target="#b1">[2]</ref>, and Gaussian Splatting <ref type="bibr" target="#b2">[3]</ref> allow anyone to create 3D content by taking photos of a real scene and optimizing a representation of that scene's underlying 3D geometry and appearance. The resulting 3D representation can then be rendered from any viewpoint, similar to traditional 3D assets. Unfortunately, creating detailed scenes still requires a labor-intensive process of capturing hundreds to thousands of photos. Captures with insufficient coverage of the scene can lead to an ill-posed optimization problem, which often results in incorrect geometry and appearance and, consequently, implausible imagery when rendering the recovered 3D model from novel viewpoints.</p><p>Reducing this requirement from dense multi-view captures to less exhaustive inputs, such as a single image or text, would enable more accessible 3D content creation. Prior work has developed specialized solutions for different input settings, such as geometry regularization techniques targeted for sparse-view reconstruction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, feed-forward models trained to create 3D objects from single images <ref type="bibr" target="#b5">[6]</ref>, or the use of image-conditioned <ref type="bibr" target="#b6">[7]</ref> or text-conditioned <ref type="bibr" target="#b7">[8]</ref> generative priors in the optimization process-but each of these specialized methods comes with associated limitations in quality, efficiency, and generality.</p><p>In this paper, we instead focus on the fundamental problem that limits the use of established 3D reconstruction methods in the observation-limited setting: an insufficient number of supervising views. Rather than devising specialized solutions for different input regimes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref>, a shared solution is to instead simply create more observations-collapsing the less-constrained, under-determined 3D creation problems to the fully-constrained, fully-observed 3D reconstruction setting. This way, we reformulate a difficult ill-posed reconstruction problem as a generation problem: given any number of input images, generate a collection of consistent novel observations of the 3D scene. Recent video generative models show promise in addressing this challenge, as they demonstrate the capability to synthesize video clips featuring plausible 3D structure <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. However, these models are often expensive to sample from, challenging to control, and limited to smooth and short camera trajectories.</p><p>Our system, CAT3D, instead accomplishes this through a multi-view diffusion model trained specifically for novel-view synthesis. Given any number of input views and any specified novel viewpoints, our model generates multiple 3D-consistent images through an efficient parallel sampling strategy. These generated images are subsequently fed through a robust 3D reconstruction pipeline to produce a 3D representation that can be rendered at interactive rates from any viewpoint. We show that our model is capable of producing photorealistic results of arbitrary objects or scenes from any number of captured or synthesized input views in as little as one minute. We evaluate our work across various input settings, ranging from sparse multi-view captures to a single captured image, and even just a text prompt (by using a text-to-image model to generate an input image from that prompt). CAT3D outperforms prior works for measurable tasks (such as the multi-view capture case) on multiple benchmarks, and is an order of magnitude faster than previous state-of-the-art. For tasks where empirical performance is difficult to measure (such as text-to-3D and single image to 3D), CAT3D compares favorably with prior work in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Creating entire 3D scenes from limited observations requires 3D generation, e.g., creating content in unseen regions, and our work builds on the ever-growing research area of 3D generative models <ref type="bibr" target="#b15">[16]</ref>. Due to the relative scarcity of 3D datasets, much research in 3D generation is centered on transferring knowledge learned by 2D image-space priors, as 2D data is abundant. Our diffusion model is built on the recent development of video and multi-view diffusion models that produce highly consistent novel views. We show that pairing these models with 3D reconstruction, similar to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b57">58]</ref>, enables efficient and high quality 3D creation. Below we discuss how our work is related to several areas of prior work.</p><p>2D priors. Given limited information such as text, pretrained text-to-image models can provide a strong generative prior for text-to-3D generation. However, distilling the knowledge present in these image-based priors into a coherent 3D model currently requires an iterative distillation approach. DreamFusion <ref type="bibr" target="#b7">[8]</ref> introduced Score Distillation Sampling (SDS) to synthesize 3D objects (as NeRFs) from text prompts. Research in this space has aimed to improve distillation strategies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, swap in other 3D representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, and amortize the optimization process <ref type="bibr" target="#b27">[28]</ref>. Using text-based priors for single-image-to-3D has also shown promise <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, but requires a complex balancing of the image observation with additional constraints. Incorporating priors such as monocular depth models or inpainting models has been useful for creating 3D scenes, but tends to result in poor global geometry <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>2D priors with camera conditioning. While text-to-image models excel at generating visually appealing images, they lack precise control over the pose of images, and thus require a time-consuming 3D distillation process to encourage the 3D model to conform to the 2D prior. To overcome this limitation, several approaches train or fine-tune generative models with explicit image and pose conditioning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. These models provide stronger priors for what an object or scene should look like given text and/or input image(s), but they also model all output views independently. In cases where there is little uncertainty in what should appear at novel views, reasoning about generated views independently is sufficient for efficient 3D reconstruction <ref type="bibr" target="#b41">[42]</ref>. But when there is some uncertainty exists, these top-performing methods still require expensive 3D distillation to resolve the inconsistencies between different novel views.</p><p>Multi-view priors. Modeling the correlations between multiple views provides a much stronger prior for what 3D content is consistent with partial observations. Methods like MVDream <ref type="bibr" target="#b42">[43]</ref>, ImageDream <ref type="bibr" target="#b8">[9]</ref>, Zero123++ <ref type="bibr" target="#b43">[44]</ref>, ConsistNet <ref type="bibr" target="#b44">[45]</ref>, SyncDreamer <ref type="bibr" target="#b45">[46]</ref> and ViewDiff <ref type="bibr" target="#b46">[47]</ref> fine-tune text-to-image models to generate multiple views simultaneously. CAT3D is similar in architecture to ImageDream, where the multi-view dependency is captured by an architecture resembling video diffusion models with 3D self-attention. Given this stronger prior, these papers also demonstrate higher quality and more efficient 3D extraction.</p><p>Video priors. Video diffusion models have demonstrated an astonishing capability of generating realistic videos <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">50]</ref>, and are thought to implicitly reason about 3D. However, it remains challenging to use off-the-shelf video diffusion models for 3D generation for a number of reasons. Current models lack exact camera controls, limiting generation to clips with only smooth and short camera trajectories, and struggle to generate videos with only camera motion but no scene dynamics. Several works have proposed to resolve these challenges by fine-tuning video diffusion models for camera-controled or multi-view generation. For example, AnimateDiff <ref type="bibr" target="#b50">[51]</ref> LoRA fine-tuned a video diffusion model with fixed types of camera motions, and MotionCtrl <ref type="bibr" target="#b51">[52]</ref> conditioned the model on arbitrary specified camera trajectories. ViVid-1-to-3 <ref type="bibr" target="#b52">[53]</ref> combines a novel view synthesis model and a video diffusion model for generating smooth trajectories. SVD-MV <ref type="bibr" target="#b53">[54]</ref>, IM-3D <ref type="bibr" target="#b16">[17]</ref> and SV3D <ref type="bibr" target="#b53">[54]</ref> further explored leveraging camera-controlled or multi-view video diffusion models for 3D generation. However, their camera trajectories are limited to orbital ones surrounding the center content. These approaches mainly focus on 3D object generation, and do not work for 3D scenes, few-view 3D reconstruction, or objects in context (objects that have not been masked or otherwise separated from the image's background).</p><p>Feed-forward methods. Another line of research is to learn feed-forward models that take a few views as input, and output 3D representations directly, without an optimization process per instance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. These methods can produce 3D representations efficiently (within a few seconds), but the quality is often worse than approaches built on image-space priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>CAT3D is a two-step approach for 3D creation: first, we generate a large number of novel views consistent with one or more input views using a multi-view diffusion model, and second, we run   (2) run a robust 3D reconstruction pipeline on the observed and generated views to learn a NeRF representation. This decoupling of the generative prior from the 3D reconstruction process results in both computational efficiency improvements and a reduction in methodological complexity relative to prior work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41]</ref>, while also yielding improved image quality.</p><formula xml:id="formula_0">c 7 G S B N G B R k Z a h g 5 q x V B f M z I 6 X j 2 L v h P P x O l q R S f z L I m J U c X g l Y U I + O h j 9 C d 7 y f 9 X r + V + K q S r p U E r O X 4 / G D 3 L 5 x I 3 H A i D G Z I 6 y L t 1 6 a 0 S B m K G X E R b D S p E Z 6 h C 1 J 4 V S B O d G n b V F 3 c 9 c g k r q T y R 5 i 4 R T c Z F n G t l 3 z s b 3 J k p v q y L 4 D X + j A S m L C t 1 + 0 c 6 a U P t g 0 G q p G S 6 W 1 4 U S m E L y V v q j e l p a J u D B F 4 l X v V s N j I O L Q y n l B F s G F L r y C s q C 8 / x l P k w x j f 8 C i C H M 0 I 8 j 9 i v B 1 B Q e Z Y c o 7 E x E L d 1 E R R X k t N X D E o C x i S k m p S 2 i S 1 r V G j l r d 5 M 7 c 2 y Z x N B s 7 L j f F y V 2 T l N p L 7 q M 5 T b + b k I Q v b j e I Y S o k Y v R C t 4 c 1 p R d k h 5 L m Z J m m S H b Y 2 x O o a 5 2 D T 6 b q R + 9 8 A a a a + / u 3 H / U g u t a / Y I K X k 3 B V p W Q z C E L W 1 K 8 J W z 8 P p W C 4 s V A 0 j B Z x Q T h a 1 g m F u g i 6 y D B q y M M H O j n p Z b a A n o k X o o b O 9 Y W 2 c W 0 X h M 6 L E 0 Z A 3 6 3 j h G z 3 J j p z 1 8 7 t 0 l j s r H A x j J 5 k d p m 1 / I 7 8 Y 6 e U 1 u K q c Z L 3 0 V e / l h 2 G S v 1 2 v y B 5 4 B p 6 D F y A F r 0 E O 3 o N j M A I Y V O A r + A a + d 7 5 0 f n R + d n 6 t r u 7 u r D l P w Z Z 0 f v 8 D v m F U X Q = = &lt; / l a t e x i t &gt; } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 J G G i a 3 P n t 2 s L L u K 1 g p E R 8 + 3 I B E = " &gt; A A A D / X i c d V N L b x M x E H Y b H m V 5 t X D k s m I b i U u j 7 C Y 8 j i u 4 c C y I t J X W q 8 p x v I 0 V P 1 a 2 l y S y L P 4 A V / g H 3 B B X f g s / g P + B v Q l S 0 s d I l m a + 8 T e e G c + M a 0 a 1 6 f f / 7 O x 2 b t 2 + c 3 f v X n T / w c N H j / c P n p x o 2 S h M R l g y q c 7 G S B N G B R k Z a h g 5 q x V B f M z I 6 X j 2 L v h P P x O l q R S f z L I m J U c X g l Y U I + O h j 9 C d 7 y f 9 X r + V + K q S r p U E r O X 4 / G D 3 L 5 x I 3 H A i D G Z I 6 y L t 1 6 a 0 S B m K G X E R b D S p E Z 6 h C 1 J 4 V S B O d G n b V F 3 c 9 c g k r q T y R 5 i 4 R T c Z F n G t l 3 z s b 3 J k p v q y L 4 D X + j A S m L C t 1 + 0 c 6 a U P t g 0 G q p G S 6 W 1 4 U S m E L y V v q j e l p a J u D B F 4 l X v V s N j I O L Q y n l B F s G F L r y C s q C 8 / x l P k w x j f 8 C i C H M 0 I 8 j 9 i v B 1 B Q e Z Y c o 7 E x E L d 1 E R R X k t N X D E o C x i S k m p S 2 i S 1 r V G j l r d 5 M 7 c 2 y Z x N B s 7 L j f F y V 2 T l N p L 7 q M 5 T b + b k I Q v b j e I Y S o k Y v R C t 4 c 1 p R d k h 5 L m Z J m m S H b Y 2 x O o a 5 2 D T 6 b q R + 9 8 A a a a + / u 3 H / U g u t a / Y I K X k 3 B V p W Q z C E L W 1 K 8 J W z 8 P p W C 4 s V A 0 j B Z x Q T h a 1 g m F u g i 6 y D B q y M M H O j n p Z b a A n o k X o o b O 9 Y W 2 c W 0 X h M 6 L E 0 Z A 3 6 3 j h G z 3 J j p z 1 8 7 t 0 l j s r H A x j J 5 k d p m 1 / I 7 8 Y 6 e U 1 u K q c Z L 3 0 V e / l h 2 G S v 1 2 v y B 5 4 B p 6 D F y A F r 0 E O 3 o N j M A I Y V O A r + A a + d 7 5 0 f n R + d n 6 t r u 7 u r D l P w Z Z 0 f v 8 D v m F U X Q = = &lt; / l a t e x i t &gt; } Ray Coordinates Ray Coordinates Multi-View Latent Diffusion Model</formula><p>a robust 3D reconstruction pipeline on the generated views (see Figure <ref type="figure" target="#fig_2">3</ref>). Below we describe our multi-view diffusion model (Section 3.1), our method for generating a large set of nearly consistent novel views from it (Section 3.2), and how these generated views are used in a 3D reconstruction pipeline (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-View Diffusion Model</head><p>We train a multi-view diffusion model that takes a single or multiple views of a 3D scene as input and generates multiple output images given their camera poses (where "a view" is a paired image and its camera pose). Specifically, given M conditional views containing M images I cond and their corresponding camera parameters p cond , the model learns to capture the joint distribution of N target images I tgt assuming their N target camera parameters p tgt are also given:</p><formula xml:id="formula_1">p I tgt |I cond , p cond , p tgt . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Model architecture. Our model architecture is similar to video latent diffusion models (LDMs) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b10">11]</ref>, but with camera pose embeddings for each image instead of time embeddings. Given a set of conditional and target images, the model encodes every individual image into a latent representation through an image variational auto-encoder <ref type="bibr" target="#b60">[61]</ref>. Then, a diffusion model is trained to estimate the joint distribution of the latent representations given conditioning signals. We initialize the model from an LDM trained for text-to-image generation similar to <ref type="bibr" target="#b61">[62]</ref> trained on web-scale image data, with an input image resolution of 512 × 512 × 3 and latents with shape 64 × 64 × 8. As is often done in video diffusion models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, the main backbone of our model remains the pretrained 2D diffusion model but with additional layers connecting the latents of multiple input images. As in <ref type="bibr" target="#b42">[43]</ref>, we use 3D self-attention (2D in space and 1D across images) instead of simple 1D self-attention across images. We directly inflate the existing 2D self-attention layers after every 2D residual block of the original LDM to connect latents with 3D self-attention layers while inheriting the parameters from the pre-trained model, introducing minimal amount of extra model parameters. We found that conditioning on input views through 3D self-attention layers removed the need for PixelNeRF <ref type="bibr" target="#b62">[63]</ref> and CLIP image embeddings <ref type="bibr" target="#b63">[64]</ref> used by the prior state-of-the-art model on few-view reconstruction, ReconFusion <ref type="bibr" target="#b6">[7]</ref>. We use FlashAttention <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref> for fast training and sampling, and fine-tune all the weights of the latent diffusion model. Similar to prior work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b66">67]</ref>, we found it important to shift the noise schedule towards high noise levels as we move from the pre-trained image diffusion model to our multi-view diffusion model that captures data of higher dimensionality. Concretely, following logic similar to <ref type="bibr" target="#b66">[67]</ref>, we shift the log signal-to-noise ratio by log(N ), where N is the number of target images. For training, latents of target images are noise perturbed while latents of conditional images are kept as clean, and the diffusion loss is defined only on target images. A binary mask is concatenated to the latents along the channel dimension, to denote conditioning vs. target images. To deal with multiple 3D generation settings, we train a single versatile model that can model a total of 8 conditioning and target views (N + M = 8), and randomly select the number of conditional views N to be 1 or 3 during training, corresponding to 7 and 5 target views respectively. See Appendix B for more model details.</p><p>Camera conditioning. To condition on the camera pose, we use a camera ray representation ("raymap") that is the same height and width as the latent representations <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b67">68]</ref> and encodes the ray origin and direction at each spatial location. The rays are computed relative to the camera pose of the first conditional image, so our pose representation is invariant to rigid transformations of 3D world coordinates. Raymaps for each image are concatenated channel-wise onto the latents for the corresponding image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generating Novel Views</head><p>Given a set of input views, our goal is to generate a large set of consistent views to fully cover the scene and enable accurate 3D reconstruction. To do this, we need to decide on the set of camera poses to sample, and we need to design a sampling strategy that can use a multi-view diffusion model trained on a small number of views to generate a much larger set of consistent views.</p><p>Camera trajectories. Compared to 3D object reconstruction where orbital camera trajectories can be effective, a challenge of 3D scene reconstruction is that the views required to fully cover a scene can be complex and depend on the scene content. We empirically found that designing reasonable camera trajectories for different types of scenes is crucial to achieve compelling few-view 3D reconstruction. The camera paths must be sufficiently thorough and dense to fully-constrain the reconstruction problem, but also must not pass through objects in the scene or view scene content from unusual angles. In summary, we explore four types of camera paths based on the characteristic of a scene: (1) orbital paths of different scales and heights around the center scene, (2) forward facing circle paths of different scales and offsets, (3) spline paths of different offsets, and (4) spiral trajectories along a cylindrical path, moving into and out of the scene. See Appendix C for more details.</p><p>Generating a large set of synthetic views. A challenge in applying our multi-view diffusion model to novel view synthesis is that it was trained with a small and finite set of input and output viewsjust 8 in total. To increase the total number of output views, we cluster the target viewpoints into smaller groups and generate each group independently given the conditioning views. We group target views with close camera positions, as these views are typically the most dependent. For single-image conditioning, we adopt an autoregressive sampling strategy, where we first generate a set of 7 anchor views that cover the scene (similar to <ref type="bibr" target="#b40">[41]</ref>, and chosen using the greedy initialization from <ref type="bibr" target="#b68">[69]</ref>), and then generate the remaining groups of views in parallel given the observed and anchor views. This allows us to efficiently generate a large set of synthetic views while still preserving both long-range consistency between anchor views and local similarity between nearby views. For the single-image setting, we generate 80 views, while for the few-view setting we use 480-960 views. See Appendix C for details.</p><p>Conditioning larger sets of input views and non-square images. To expand the number of views we can condition on, we choose the nearest M views as the conditioning set, as in <ref type="bibr" target="#b6">[7]</ref>. We experimented with simply increasing the sequence length of the multi-view diffusion architecture during sampling, but found that the nearest view conditioning and grouped sampling strategy performed better. To handle wide aspect ratio images, we combine square samples from square-cropped input views with wide samples cropped from input views padded to be square.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input ReconFusion [7] CAT3D (ours) Ground Truth</head><p>Figure <ref type="figure">4</ref>: Qualitative comparison of few-view reconstruction on scenes from the mip-NeRF 360 <ref type="bibr" target="#b72">[73]</ref> and CO3D <ref type="bibr" target="#b73">[74]</ref> datasets. Samples shown here are rendered images, with 3 input captured views. Compared to baseline approaches like ReconFusion <ref type="bibr" target="#b6">[7]</ref>, CAT3D aligns with ground truth well in seen regions, while hallucinating plausible content in unseen regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Robust 3D reconstruction</head><p>Our multi-view diffusion model generates a set of high-quality synthetic views that are reasonably consistent with each other. However, the generated views are generally not perfectly 3D consistent. Indeed, generating perfectly 3D consistent images remains a very challenging problem even for current state-of-the-art video diffusion models <ref type="bibr" target="#b69">[70]</ref>. Since 3D reconstruction methods have been designed to take photographs (which are by definition perfectly consistent) as input, we modify the standard NeRF training procedure to improve its robustness to inconsistent input views.</p><p>We build upon Zip-NeRF <ref type="bibr" target="#b70">[71]</ref>, whose training procedure minimizes the sum of a photometric reconstruction loss, a distortion loss, an interlevel loss, and a normalized L2 weight regularizer. We additionally include a perceptual loss (LPIPS <ref type="bibr" target="#b71">[72]</ref>) between the rendered image and the input image. Compared to the photometric reconstruction loss, LPIPS emphasizes high-level semantic similarity between the rendered and observed images, while ignoring potential inconsistencies in low-level high-frequency details. Since generated views closer to the observed views tend to have less uncertainty and are therefore more consistent, we weight the losses for generated views based on the distance to the nearest observed view. This weighting is uniform at the beginning of the training, and is gradually annealed to a weighting function that more strongly penalizes reconstruction losses for views closer to one of the observed views. See Appendix D for additional details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We trained the multi-view diffusion model at the core of CAT3D on four datasets with camera pose annotations: Objaverse <ref type="bibr" target="#b74">[75]</ref>, CO3D <ref type="bibr" target="#b73">[74]</ref>, RealEstate10k <ref type="bibr" target="#b75">[76]</ref> and MVImgNet <ref type="bibr" target="#b76">[77]</ref>. We then evaluated CAT3D on the few-view reconstruction task (Section 4.1) and the single image to 3D task (Section 4.2), demonstrating qualitative and quantitative improvements over prior work. The design choices that led to CAT3D are ablated and discussed further in Section 4.3.   <ref type="table">1</ref>: Quantitative comparison of few-view 3D reconstruction. CAT3D outperforms baseline approaches across nearly all settings and metrics (modified baselines denoted with * taken from <ref type="bibr" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>← ----------------------------------------→</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Few-View 3D Reconstruction</head><p>We first evaluate CAT3D on five real-world benchmark datasets for few-view 3D reconstruction. Among those, CO3D <ref type="bibr" target="#b73">[74]</ref> and RealEstate10K <ref type="bibr" target="#b75">[76]</ref> are in-distribution datasets whose training splits were part of our training set (we use their test splits for evaluation), whereas DTU <ref type="bibr" target="#b77">[78]</ref>, LLFF <ref type="bibr" target="#b78">[79]</ref> and the mip-NeRF 360 dataset <ref type="bibr" target="#b72">[73]</ref> are out-of-distribution datasets that were not part of the training dataset.</p><p>We tested CAT3D on the 3, 6 and 9 view reconstruction tasks, with the same train and eval splits as <ref type="bibr" target="#b6">[7]</ref>.</p><p>In Table <ref type="table">1</ref>, we compare to the state-of-the-art for dense-view NeRF reconstruction with no learned priors (Zip-NeRF <ref type="bibr" target="#b70">[71]</ref>) and methods that heavily leverage generative priors such as ZeroNVS <ref type="bibr" target="#b40">[41]</ref> and ReconFusion <ref type="bibr" target="#b6">[7]</ref>. We find that CAT3D achieves state-of-the-art performance across nearly all settings, while also reducing generation time from 1 hour (for ZeroNVS and ReconFusion) down to a few minutes. CAT3D outperforms baseline approaches on more challenging datasets like CO3D and mip-NeRF 360 by a larger margin, thereby demonstrating its value in reconstructing large and highly detailed scenes. Figure <ref type="figure">4</ref> shows the qualitative comparison. In unobserved regions, CAT3D is able to hallucinate plausible textured content while still preserving geometry and appearance from the input views, whereas prior works often produce blurry details and oversmoothed backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Single image to 3D</head><p>CAT3D supports the efficient generation of diverse 3D content from just a single input view. Evaluation in this under-constrained regime is challenging as there are many 3D scenes consistent with the single view, for example scenes of different scales. We thus focus our single image evaluation on qualitative comparisons (Figure <ref type="figure" target="#fig_4">5</ref>), and quantitative semantic evaluations with CLIP <ref type="bibr" target="#b63">[64]</ref> (Table <ref type="table">2</ref>). On scenes, CAT3D produces higher resolution results than ZeroNVS <ref type="bibr" target="#b40">[41]</ref> and RealmDreamer <ref type="bibr" target="#b79">[80]</ref>, and for both scenes and objects we better preserve details from the input image. On images with segmented objects, our geometry is often worse than existing approaches like ImageDream <ref type="bibr" target="#b8">[9]</ref> and DreamCraft3D <ref type="bibr" target="#b80">[81]</ref>, but maintains competitive CLIP scores. Compared to these prior approaches that iteratively leverage a generative prior in 3D distillation, CAT3D is more than an order of magnitude faster. Faster generation methods have been proposed for objects <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83]</ref>, but produce significantly lower resolution results than their iterative counterparts, so they are not included in this comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input CAT3D (ours)</head><p>RealmDreamer <ref type="bibr" target="#b79">[80]</ref> ZeroNVS <ref type="bibr" target="#b40">[41]</ref> ImageDream <ref type="bibr" target="#b8">[9]</ref> DreamCraft3D <ref type="bibr" target="#b80">[81]</ref> Baselines Table <ref type="table">2</ref>: Evaluating image-to-3D quality with CLIP image scores on examples from <ref type="bibr" target="#b8">[9]</ref> (numbers reproduced from <ref type="bibr" target="#b16">[17]</ref>). CAT3D produces competitive results to object-centric baselines while also working on whole scenes.</p><p>IM-3D <ref type="bibr" target="#b16">[17]</ref> achieves better performance on segmented objects with similar runtime, but does not work on scenes, or on objects in context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>At the core of CAT3D is a multi-view diffusion model that has been trained to generate consistent novel views. We considered several different model variants, and evaluated both their sample quality (on in-domain and out-of-domain datasets) and few-view 3D reconstruction performance. We also compare important design choices for 3D reconstruction. Results from our ablation study are reported in Table <ref type="table">3</ref> and Figure <ref type="figure" target="#fig_5">6</ref> in Appendix A and summarized below. Overall, we found that video diffusion architectures, with 3D self-attention (spatiotemporal) and raymap embeddings of camera pose, produce consistent enough views to recover 3D representations when combined with robust reconstruction losses.</p><p>Image and pose. Previous work <ref type="bibr" target="#b6">[7]</ref> used PixelNerf <ref type="bibr" target="#b62">[63]</ref> feature-map conditioning for multiple input views. We found that replacing PixelNeRF with attention-based conditioning in a conditional video diffusion architecture using a per-image embedding of the camera pose results in improved samples and 3D reconstructions, while also reducing model complexity and the number of parameters.</p><p>We found that embedding the camera pose as a low-dimensional vector (as in <ref type="bibr" target="#b35">[36]</ref>) works well for in-domain samples, but generalizes poorly compared to raymap conditioning (see Section 3.1).</p><p>Increasing the number of views. We found that jointly modeling multiple output views (i.e., 5 or 7 views instead of 1) improves sample metrics -even metrics that evaluate the quality of each output image independently. Jointly modeling multiple outputs creates more consistent views that result in an improved 3D reconstruction as well.</p><p>Attention layers. We found that 3D self-attention (spatiotemporal) is crucial, as it yields improved performance relative to factorized 2D self-attention (spatial-only) and 1D self-attention (temporalonly). While models with 3D self-attention in the finest feature maps (64 × 64) result in the highest fidelity images, they incur a significant computational overhead for training and sampling for relative small gain in fidelity. We therefore decided to use 3D self-attention only in feature maps of size 32 × 32 and smaller.</p><p>Multi-view diffusion model training. Initializing from a pre-trained text-to-image latent diffusion model improved performance on out-of-domain examples. We experimented with fine-tuning the multi-view diffusion model to multiple variants specialized for specific numbers of inputs and outputs views, but found that a single model jointly trained on 8 frames with either 1 or 3 conditioning views was sufficient to enable accurate single image and few-view 3D reconstruction.</p><p>3D reconstruction. LPIPS loss is crucial for achieving high-quality texture and geometry, aligning with findings in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7]</ref>. On Mip-NeRF 360, increasing the number of generated views from 80 (single elliptical orbit) to 720 (nine orbits) improved central object geometry but sometimes introduced background blur, probably due to inconsistencies in generated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We present CAT3D, a unified approach for 3D content creation from any number of input images. CAT3D leverages a multi-view diffusion model for generating highly consistent novel views of a 3D scene, which are then input into a 3D multi-view reconstruction pipeline. CAT3D decouples the generative prior from 3D extraction, leading to efficient, simple, and high-quality 3D generation.</p><p>Although CAT3D produces compelling results and outperforms prior works on multiple tasks, it has limitations. Because our training datasets have roughly constant camera intrinsics for views of the same scene, the trained model cannot handle test cases well where input views are captured by multiple cameras with different intrinsics. The generation quality of CAT3D relies on the expressivity of the base text-to-image model, and it performs worse in the cases where scene content is out of distribution for the base model. The number of output views supported by our multi-view diffusion model is still relatively small, so when we generate a large set of samples from our model, not all views may be 3D consistent with each other. Finally, CAT3D uses manually-constructed camera trajectories that cover the scene thoroughly (see Appendix C), which may be difficult to design for large-scale open-ended 3D environments.</p><p>There are a few directions worth exploring in future work to improve CAT3D. The multi-view diffusion model may benefit from being initialized from a pre-trained video diffusion model, as observed by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>. The consistency of samples could be further improved by extending the number of conditioning and target views handled by the model. Automatically determining the camera trajectories required for different scenes could increase the flexibility of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ablations</head><p>Here we conduct ablation studies over several important decisions that led to CAT3D. We consider several multi-view diffusion model variants, and evaluated them on novel-view synthesis with held-out validation sets (4k samples) from the training datasets (in-domain samples) and from the mip-NeRF 360 dataset (out-of-domain samples), as well as 3-view reconstruction on the mip-NeRF 360 dataset (out-of-domain renders). Unless otherwise specified, models in this section are trained with 3 input views and 5 output views, evaluated after 120k optimization iterations. Quantitative results are summarized in Table <ref type="table">3</ref>. Then we discuss important 3D reconstruction design choices, with qualitative comparison in Figure <ref type="figure" target="#fig_5">6</ref>.</p><p>Number of target views. We start from the setting where the model takes 3 conditional views as input and generates a single target view, identical to the ReconFusion baseline <ref type="bibr" target="#b6">[7]</ref>. Results show that our multi-view architecture is favored when dealing with multiple input views, compared to the PixelNeRF used by <ref type="bibr" target="#b6">[7]</ref>. We also show that going from a single target view to 5 target views results in a significant improvement on both novel-view synthesis and few-view reconstruction.</p><p>Camera conditioning. As mentioned in Section 3.1, we compared two camera parameterizations, one is an 8-dimensional encoding vector fed through cross-attention layers that contains relative position, relative rotation quaternion, and absolute focal length. The other is camera rays fed through channel-wise concatenation with the input latents. We observe that the latter performs better across all metrics. We fine-tune the full latent diffusion model for 1.4M iterations with a batch size of 128 and a learning rate of 5 × 10 -5 . The first 1M iterations are trained with the setting of 1 conditional view and 7 target views, while the rest 0.4M iterations are trained with an equal mixture of 1 condition + 7 target views and 3 conditional + 5 target views. Following <ref type="bibr" target="#b6">[7]</ref> we draw training samples with equal probability from the four training datasets. We enable classifier-free guidance (CFG) <ref type="bibr" target="#b84">[85]</ref> by randomly dropping the conditional images and camera poses with a probability of 0.1 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Generating Novel Views</head><p>We use DDIM <ref type="bibr" target="#b85">[86]</ref> with 50 sampling steps and CFG guidance weight 3 for generating novel views. It takes 5 seconds to generate 80 views on 16 A100 GPUs. As mentioned in Section 3.2, selecting camera trajectories that fully cover the 3D scene is important for high-quality 3D generation results. See Figure <ref type="figure" target="#fig_6">8</ref> for an illustration of the camera trajectories we use. For the single image-to-3D setting we use two different types of camera trajectories, each containing 80 views:</p><p>• A spiral around a cylinder-like trajectory that moves into and out of the scene.</p><p>• An orbit trajectory for images with a central object.</p><p>For few-view reconstruction, we create different trajectories based on the characteristics of different datasets:</p><p>• RealEstate10K: we create a spline path fitted from the input views, and shift the trajectories along the xz-plane by certain offsets, resulting in 800 views. of the text-to-image model to be 3D self-attention (spatiotemporal). We remove the text embedding conditioning of the original model.</p><p>• LLFF and DTU: we create a forward-facing circle path fitted from all views in the training set, scale it, and shift along the z-axis by certain offsets, resulting in 960 and 480 views respectively. • CO3D: we create a spline path fitted from the input views, and scale the trajectories by multiple factors, resulting in 640 views. • Mip-NeRF 360: we create a elliptical path fitted from all views in the training set , scale it, and shift along the z-axis by certain offsets, resulting in 720 views.</p><p>For generating anchor views in the single-image setting, we used the model with 1 conditional input and 7 target outputs. For generating the full set of views (both in the single-image anchored setting, as well as in the multi-view setting), we used the model with 3 conditional inputs and groups of 5 target outputs, selected by their indices in the camera trajectories.</p><p>Anchor selection. For single-image conditioning, we first select a set of target viewpoints as anchors to ground the scene content. To select a set of anchor views that are spread throughout the scene and provide good coverage, we use the initialization strategy from <ref type="bibr" target="#b68">[69]</ref>. This method greedily selects the camera whose position is furthest away from the already selected views. We found this to work well on a variety of trajectory-like view sets as well as random views that have been spread throughout a scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of 3D Reconstruction</head><p>For the Zip-NeRF <ref type="bibr" target="#b70">[71]</ref> baseline, we follow <ref type="bibr" target="#b6">[7]</ref> to make a few modifications of the hyperparameters (See Appendix D in <ref type="bibr" target="#b6">[7]</ref>) that better suit the few-view reconstruction setting. We use a smaller view dependence network with width 32 and depth 1, and a smaller number of training iterations of 1000, which helps avoid overfitting and substantially speeds up the training and rendering. Our synthetic view sampling and 3D reconstruction process is run on 16 A100 GPUs. For few-view reconstruction, we sample 128 × 128 patches of rays and train with a global batch size of 1M that takes 4 minutes to train. For single image to 3D, we use 32 × 32 patches of rays and a global batch size of 65k that takes 55 seconds to train. Learning rate is logarithmically decayed from 0.04 to 10 -3 . The weight of the perceptual loss (LPIPS) is set to 0.25 for single image to 3D and few-view reconstruction on RealState10K, LLFF and DTU datasets, and to 1.0 for few-view reconstruction on CO3D an MipNeRF-360 dataets.</p><note type="other">18</note><p>Distance based weighting. We design a weighting schedule that upweights views closer to captured views in the later stage of training to improve details. Specifically, the weighting is given by a Gaussian kernel: w ∝ exp -bs 2 , where s is the distance to the closest captured view and b is a scaling factor. For few-view reconstruction, b is linearly annealed from 0 to 15. We also anneal the weighting of generated views globally to further emphasize the importance of captured views.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Qualitative results: CAT3D can create high-quality 3D objects or scenes from a number of input modalities: an input image generated by a text-to-image model (rows 1-2), a single captured real image (rows 3-4), and multiple captured real images (row 5).</figDesc><graphic coords="4,26.11,589.69,277.20,138.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 4 J G G i a 3 P n t 2 s L L u K 1 g p E R 8 + 3 I B E = " &gt; A A A D / X i c d V N L b x M x E H Y b H m V 5 t X D k s m I b i U u j 7 C Y 8 j i u 4 c C y I t J X W q 8 p x v I 0 V P 1 a 2 l y S y L P 4 A V / g H 3 B B X f g s / g P + B v Q l S 0 s d I l m a + 8 T e e G c + M a 0 a 1 6 f f / 7 O x 2 b t 2 + c 3 f v X n T / w c N H j / c P n p x o 2 S h M R l g y q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the method. Given one to many views, CAT3D creates a 3D representation of the entire scene in as little as one minute. CAT3D has two stages: (1) generate a large set of synthetic views from a multi-view latent diffusion model conditioned on the input views alongside the camera poses of target views;(2) run a robust 3D reconstruction pipeline on the observed and generated views to learn a NeRF representation. This decoupling of the generative prior from the 3D reconstruction process results in both computational efficiency improvements and a reduction in methodological complexity relative to prior work<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41]</ref>, while also yielding improved image quality.</figDesc><graphic coords="5,218.63,72.73,280.51,220.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Method PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓Harder Dataset DifficultyEasier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: 3D creation from single input images. Renderings of 3D models from CAT3D (middle row) are higher quality than baselines (bottom row) for scenes, and competitive for objects. Note that scale ambiguity amplifies the differences in renderings between methods.</figDesc><graphic coords="9,308.33,279.60,87.87,87.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Qualitative comparison of 3D reconstruction design choices. Rendered images (left) and depth maps (right) of a Mip-NeRF 360 scene under different settings: (a) 720 generated views along multiple orbital paths, (b) 80 generated views on a single orbital path, and (c) 720 views, without the perceptual (LPIPS) loss.</figDesc><graphic coords="17,160.95,283.88,141.72,91.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Camera trajectories for generating novel views. Within each panel, left shows the side view and right shows the top view of the trajectories, colored by indices of views. (a)-(b): two types of trajectories used by single image to 3D. Observed view is highlighted in red, while anchor views are highlighted in orange. (c)-(g): trajectories used by 3D reconstruction. 3 input views are highlighted in red.</figDesc><graphic coords="19,113.98,477.18,87.86,75.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Illustration of the network. CAT3D builds on a latent text-to-image diffusion model. The input images of size 512 are 8× downsampled to latents of size 64 × 64, which are concatenated with the relative camera raymap and a binary mask that indicates whether or not the image has been observed. A 2D U-Net with temporal connections is utilized for building the latent diffusion model. After each residual block with resolution ≤ 32 × 32, we inflate the original 2D self-attention (spatial)</figDesc><table><row><cell>8×512×512</cell><cell>Input Images</cell><cell></cell><cell>Output Images</cell><cell></cell></row><row><cell>Relative camera raymap</cell><cell>Encoder</cell><cell></cell><cell>Decoder</cell><cell></cell></row><row><cell>Observed mask</cell><cell></cell><cell></cell><cell></cell><cell>2x</cell></row><row><cell>8×64×64</cell><cell>Downsampling Block</cell><cell>Skip</cell><cell>Upsampling Block</cell><cell>Spatial Conv2D Stack</cell></row><row><cell></cell><cell>1/2×</cell><cell></cell><cell>2×</cell><cell>Spatiotemporal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Joint Attention</cell></row><row><cell>8×32×32</cell><cell>Downsampling Block</cell><cell>Skip</cell><cell>Upsampling Block*</cell><cell></cell></row><row><cell></cell><cell>1/2×</cell><cell></cell><cell>2×</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Joint Attention</cell></row><row><cell>8×16×16</cell><cell>Downsampling Block</cell><cell>Skip</cell><cell>Upsampling Block*</cell><cell>B T H W C -&gt; B (T H W) C Flash Attention</cell></row><row><cell></cell><cell>1/2×</cell><cell></cell><cell>2×</cell><cell>B (T H W) C -&gt; B T H W C</cell></row><row><cell>8×8×8</cell><cell></cell><cell>Middle Conv2D Stack</cell><cell></cell><cell></cell></row><row><cell>Figure 7:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements We would like to thank <rs type="person">Daniel Watson</rs>, <rs type="person">Rundi Wu</rs>, <rs type="person">Richard Tucker</rs>, <rs type="person">Jason Baldridge</rs>, <rs type="person">Michael Niemeyer</rs>, <rs type="person">Rick Szeliski</rs>, <rs type="person">Jordi Pont-Tuset</rs>, <rs type="person">Andeep Torr</rs>, <rs type="person">Irina Blok</rs>, <rs type="person">Doug Eck</rs>, and <rs type="person">Henna Nandwani</rs> for their valuable contributions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention layers. An important design choice is the type and number of self-attention layers used when connecting multiple views. As shown in Table <ref type="table">3</ref>, it is critical to use 3D self-attention instead of temporal 1D self-attention. However, 3D attention is expensive; 3D attention at the largest feature maps (64 × 64) is of 32k sequence length and this incurs a significant computation overhead during training and sampling with a marginal performance gain, especially for out-of-domain samples and renderings. We therefore chose to use 3D attention only for feature maps of size 32 × 32 and smaller.</p><p>Multi-view diffusion model training. We compared the settings of training the multi-view diffusion models from scratch with initializing from a pre-trained text-to-image latent diffusion model. The latter performs better, especially for out-of-domain cases. We further trained the model for more iterations and observed a consistent performance gain up until 1M iterations. Then we fine-tuned the model for handling both cases of 1 conditional + 7 target views and 3 conditional + 5 target views (jointly), for another 0.4M iterations. We found this joint finetuning leads to better in-domain novel-view synthesis results with 3 conditional views, and out-of-domain results that are on-par with the previous model.</p><p>3D reconstruction. We found perceptual distance (LPIPS) loss is crucial in recovering high-quality texture and geometry, similar to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7]</ref>. We also compared the use of 80 views along one orbital path with 720 views along nine variably scaled orbital paths. In the Mip-NeRF 360 setting, increasing the number of views helps better regularize the geometry of central objects, but sometimes leads to blurrier textures in the background, due to inconsistencies in generated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Multi-View Diffusion Model</head><p>We initialize the multi-view diffusion model from a latent diffusion model (LDM) trained for text-toimage generation similar to <ref type="bibr" target="#b61">[62]</ref> trained on web scale image datasets. See Figure <ref type="figure">7</ref> for a visualization of the model architecture. We modify the LDM to take multi-view images as input by inflating the 2D self-attention after every 2D residual blocks to 3D self-attention <ref type="bibr" target="#b42">[43]</ref>. Our model adds minimal additional parameters to the backbone model: just a few additional convolution channels at the input layer to handle conditioning information. We drop the text embedding from the original model. Our latent diffusion model has 850M parameters, smaller than existing approaches built on video diffusion models such as IM-3D <ref type="bibr" target="#b16">[17]</ref> (4.3B) and SV3D <ref type="bibr" target="#b53">[54]</ref> (1.5B).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Instant neural graphics primitives with a multiresolution hash encoding</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D Gaussian Splatting for Real-Time Radiance Field Rendering</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Kerbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Kopanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leimkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">SimpleNeRF: Regularizing Sparse Input Neural Radiance Fields with Simpler Solutions</title>
		<author>
			<persName><forename type="first">Nagabhushan</forename><surname>Somraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adithyan</forename><surname>Karanayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>SIGGRAPH Asia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">LRM: Large Reconstruction Model for Single Image to 3D</title>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.04400</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Rundi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<title level="m">Reconfusion: 3d reconstruction with diffusion priors</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">DreamFusion: Text-to-3D using 2D Diffusion</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Imagedream: Image-prompt multi-view diffusion for 3d generation</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02201</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Align your latents: High-resolution video synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akbar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10709</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Lumiere: A space-time diffusion model for video generation</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roni</forename><surname>Paiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiran</forename><surname>Zada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Photorealistic video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meera</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">State of the art on diffusion models for visual computing</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Ryan Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07204</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippos</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Dream-Time: An Improved Optimization Strategy for Text-to-3D Content Creation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity</title>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreyas</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collaborative score distillation for consistent visual editing</title>
		<author>
			<persName><forename type="first">Subin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">June</forename><forename type="middle">Suk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</title>
		<author>
			<persName><forename type="first">Zhengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Instruct-nerf2nerf: Editing 3d scenes with instructions</title>
		<author>
			<persName><forename type="first">Ayaan</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19740" to="19750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningxin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Magic3D: High-Resolution Text-to-3D Content Creation</title>
		<author>
			<persName><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16653</idno>
		<title level="m">Dreamgaussian: Generative gaussian splatting for efficient 3d content creation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors</title>
		<author>
			<persName><forename type="first">Taoran</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08529</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Dave</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16936</idno>
		<title level="m">Disentangled 3d scene generation with layout learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ATT3D: Amortized Text-to-3D Object Synthesis</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Lorraine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Realfusion: 360deg reconstruction of any object from a single image</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors</title>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.17843</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior</title>
		<author>
			<persName><forename type="first">Junshu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Höllein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Monocular depth estimation using diffusion models</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.14816</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">WonderJourney: Going from Anywhere to Everywhere</title>
		<author>
			<persName><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>William T Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03884</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Nerfiller: Completing scenes via generative 3d inpainting</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Hołyński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.04560</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Zero-1-to-3: Zero-Shot One Image to 3D Object</title>
		<author>
			<persName><forename type="first">Ruoshi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rundi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basile</forename><surname>Van Hoorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Novel view synthesis with diffusion models</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.04628</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DreamBooth3D: Subject-Driven Text-to-3D Generation</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Kaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiran</forename><surname>Zada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Trevithick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-En</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">GeNVS: Generative novel view synthesis with 3D-aware diffusion models</title>
		<author>
			<persName><forename type="first">Koki</forename><surname>Eric R Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong Joon</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalini</forename><forename type="middle">De</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><surname>Wetzstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Image</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Ryan Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization</title>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haian</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">MVDream: Multi-view Diffusion for 3D Generation</title>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianglong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Zero123++: a single image to consistent multi-view diffusion base model</title>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion</title>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10343</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">SyncDreamer: Generating Multiview-consistent Images from a Single-view Image</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijiao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Viewdiff: 3d-consistent image generation with text-to-image models</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Höllein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aljaž</forename><surname>Božič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Video diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03458</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Imagen video: High definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tabellion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Hołyński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Kontkanen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.01203</idno>
		<title level="m">Video interpolation with diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04725</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Motionctrl: A unified and flexible motion controller for video generation</title>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03641</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models</title>
		<author>
			<persName><forename type="first">Jeong-Gi</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erqun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhe</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanseok</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang</forename><surname>Moo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2312.01305</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion</title>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Han</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Boss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pankratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Tochilkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Laforte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">3DGen: Triplane Latent Diffusion for Textured Mesh Generation</title>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05371</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Szymanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model</title>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.06214</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Szymanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.13150</idno>
		<title level="m">Splatter image: Ultra-fast single-view 3d reconstruction</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbo</forename><surname>Xiangli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.19702</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis with Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">pixelNeRF: Neural Radiance Fields from One or Few Images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vickie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Flashattention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08691</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Simple diffusion: End-to-end diffusion for high resolution images</title>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noha</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhani</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">k-means++: the advantages of careful seeding</title>
		<author>
			<persName><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Shadows Don&apos;t Lie and Lines Can&apos;t Bend! Generative Models don&apos;t know Projective Geometry</title>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amitabh</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Bhattad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.17138</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</title>
		<author>
			<persName><forename type="first">Jonathan T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pratul P Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Hedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</title>
		<author>
			<persName><forename type="first">Jonathan T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pratul P Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Hedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Sbordone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Objaverse: A universe of annotated 3d objects</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Deitke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">MVImgNet: A Large-scale Dataset of Multi-view Images</title>
		<author>
			<persName><forename type="first">Xianggang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haolin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyou</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Large scale multi-view stereopsis evaluation</title>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Ortiz-Cayon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Khademi Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion</title>
		<author>
			<persName><forename type="first">Jaidev</forename><surname>Shriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Trevithick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior</title>
		<author>
			<persName><forename type="first">Jingxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers</title>
		<author>
			<persName><forename type="first">Zi-Xin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Chen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan-Pei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Triposr: Fast 3d object reconstruction from a single image</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Tochilkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pankratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Laforte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan-Pei</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion</title>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07885</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
