// Background document processing for RAG system
// Phase 2: Chunking & Embedding Pipeline

import { createClient } from '@/lib/supabase/server'
import { chunkText, ChunkingOptions, CHUNKING_PRESETS } from './chunking'
import { EmbeddingService, EmbeddingResult } from './embeddings'
import { trackDatabaseQuery } from '@/lib/performance'
import { RAGDocument, RAGChunk, RAGIngestJob } from './types'

export interface ProcessingOptions {
  chunkingOptions?: ChunkingOptions
  embeddingModel?: string
  updateJobStatus?: boolean
}

export interface ProcessingResult {
  success: boolean
  documentId: string
  chunksCreated: number
  totalTokens: number
  processingTimeMs: number
  error?: string
}

export interface ProcessingStats {
  documentsProcessed: number
  chunksCreated: number
  totalTokens: number
  averageProcessingTime: number
  errors: string[]
}

// Main document processing function
export async function processDocument(
  documentId: string,
  content: string,
  userId: string,
  options: ProcessingOptions = {}
): Promise<ProcessingResult> {
  const startTime = performance.now()
  const supabase = await createClient()
  
  try {
    console.log(`Starting processing for document ${documentId}`)
    
    // Step 1: Chunk the document
    const chunkingResult = chunkText(content, options.chunkingOptions || CHUNKING_PRESETS.balanced)
    console.log(`Created ${chunkingResult.chunks.length} chunks from document ${documentId}`)
    
    if (chunkingResult.chunks.length === 0) {
      throw new Error('No chunks generated from document content')
    }
    
    // Step 2: Generate embeddings for all chunks
    const embeddingService = new EmbeddingService(options.embeddingModel)
    const chunkContents = chunkingResult.chunks.map(chunk => chunk.content)
    const embeddingResults = await embeddingService.generateBatchEmbeddings(chunkContents)
    
    console.log(`Generated embeddings for ${embeddingResults.length} chunks`)
    
    if (embeddingResults.length === 0) {
      throw new Error('Failed to generate any embeddings for document chunks')
    }
    
    // Step 3: Prepare chunk records for database insertion
    const chunkRecords = chunkingResult.chunks.map((chunk, index) => {
      const embedding = embeddingResults[index]
      
      if (!embedding) {
        console.warn(`No embedding found for chunk ${index}, skipping`)
        return null
      }
      
      return {
        doc_id: documentId,
        chunk_index: chunk.index,
        content: chunk.content,
        embedding: embedding.embedding,
        // fts will be auto-generated by trigger
        tags: [], // Inherit from document or set based on content analysis
        labels: {}, // Additional metadata can be added here
      }
    }).filter(Boolean) // Remove null entries
    
    if (chunkRecords.length === 0) {
      throw new Error('No valid chunk records to insert')
    }
    
    // Step 4: Insert chunks into database
    const dbStartTime = performance.now()
    const { error: insertError } = await supabase
      .from('rag_chunks')
      .insert(chunkRecords)
    
    trackDatabaseQuery('rag_chunks_insert', dbStartTime)
    
    if (insertError) {
      console.error('Failed to insert chunks:', insertError)
      throw new Error(`Database insertion failed: ${insertError.message}`)
    }
    
    const processingTime = performance.now() - startTime
    
    console.log(`Successfully processed document ${documentId} in ${processingTime.toFixed(2)}ms`)
    
    return {
      success: true,
      documentId,
      chunksCreated: chunkRecords.length,
      totalTokens: embeddingResults.reduce((sum, result) => sum + result.tokens, 0),
      processingTimeMs: processingTime
    }
    
  } catch (error) {
    const processingTime = performance.now() - startTime
    const errorMessage = error instanceof Error ? error.message : 'Unknown processing error'
    
    console.error(`Failed to process document ${documentId}:`, errorMessage)
    
    return {
      success: false,
      documentId,
      chunksCreated: 0,
      totalTokens: 0,
      processingTimeMs: processingTime,
      error: errorMessage
    }
  }
}

// Process a document from an ingestion job
export async function processDocumentJob(jobId: string): Promise<ProcessingResult> {
  const supabase = await createClient()
  
  try {
    // Get the job details
    const { data: job, error: jobError } = await supabase
      .from('rag_ingest_jobs')
      .select('*')
      .eq('id', jobId)
      .single()
    
    if (jobError || !job) {
      throw new Error(`Job not found: ${jobError?.message || 'Unknown error'}`)
    }
    
    if (job.status !== 'queued') {
      throw new Error(`Job ${jobId} is not in queued status: ${job.status}`)
    }
    
    // Update job status to processing
    await updateJobStatus(jobId, 'processing')
    
    // Extract document ID and operation from job payload
    const payload = job.payload as Record<string, unknown>
    const documentId = payload.document_id as string
    const operation = payload.operation as string
    
    if (!documentId || operation !== 'chunk_and_embed') {
      throw new Error(`Invalid job payload: ${JSON.stringify(payload)}`)
    }
    
    // Get the document
    const { data: document, error: docError } = await supabase
      .from('rag_documents')
      .select('*')
      .eq('id', documentId)
      .eq('owner', job.owner)
      .single()
    
    if (docError || !document) {
      throw new Error(`Document not found: ${docError?.message || 'Unknown error'}`)
    }
    
    // Get the document content (for now, assume it's in the payload)
    const content = payload.content as string
    if (!content) {
      throw new Error('Document content not found in job payload')
    }
    
    // Process the document
    const result = await processDocument(documentId, content, job.owner, {
      chunkingOptions: CHUNKING_PRESETS.balanced,
      updateJobStatus: false // We handle status updates here
    })
    
    // Update job status based on result
    if (result.success) {
      await updateJobStatus(jobId, 'completed', undefined, {
        chunksCreated: result.chunksCreated,
        totalTokens: result.totalTokens,
        processingTimeMs: result.processingTimeMs
      })
    } else {
      await updateJobStatus(jobId, 'error', result.error)
    }
    
    return result
    
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : 'Unknown job processing error'
    
    // Update job status to error
    try {
      await updateJobStatus(jobId, 'error', errorMessage)
    } catch (statusError) {
      console.error('Failed to update job status:', statusError)
    }
    
    return {
      success: false,
      documentId: '',
      chunksCreated: 0,
      totalTokens: 0,
      processingTimeMs: 0,
      error: errorMessage
    }
  }
}

// Update job status helper
async function updateJobStatus(
  jobId: string, 
  status: 'queued' | 'processing' | 'completed' | 'error',
  error?: string,
  results?: Record<string, unknown>
): Promise<void> {
  const supabase = await createClient()
  
  const updateData: Record<string, unknown> = {
    status,
    updated_at: new Date().toISOString()
  }
  
  if (error) {
    updateData.error = error
  }
  
  if (results) {
    // Merge results into payload
    const { data: currentJob } = await supabase
      .from('rag_ingest_jobs')
      .select('payload')
      .eq('id', jobId)
      .single()
    
    if (currentJob) {
      updateData.payload = {
        ...(currentJob.payload as Record<string, unknown>),
        results
      }
    }
  }
  
  const dbStartTime = performance.now()
  const { error: updateError } = await supabase
    .from('rag_ingest_jobs')
    .update(updateData)
    .eq('id', jobId)
  
  trackDatabaseQuery('rag_jobs_update', dbStartTime)
  
  if (updateError) {
    console.error(`Failed to update job ${jobId} status:`, updateError)
    throw new Error(`Failed to update job status: ${updateError.message}`)
  }
}

// Enhanced document upload API that triggers processing
export async function uploadAndProcessDocument(
  title: string,
  content: string,
  sourceType: 'text' | 'url' | 'pdf' | 'docx',
  userId: string,
  sourceUri?: string,
  docDate?: string,
  tags: string[] = [],
  labels: Record<string, unknown> = {}
): Promise<{
  document: RAGDocument
  job: RAGIngestJob
}> {
  const supabase = await createClient()
  
  // Parse and validate doc_date
  let documentDate: Date
  if (docDate) {
    documentDate = new Date(docDate)
    if (isNaN(documentDate.getTime())) {
      throw new Error('Invalid doc_date format')
    }
  } else {
    documentDate = new Date()
  }
  
  // Insert document
  const dbStartTime = performance.now()
  const { data: document, error: docError } = await supabase
    .from('rag_documents')
    .insert({
      owner: userId,
      title,
      source_type: sourceType,
      source_uri: sourceUri,
      doc_date: documentDate.toISOString().split('T')[0],
      tags,
      labels
    })
    .select('*')
    .single()
  
  trackDatabaseQuery('rag_documents_insert', dbStartTime)
  
  if (docError || !document) {
    throw new Error(`Failed to create document: ${docError?.message}`)
  }
  
  // Create processing job
  const jobPayload = {
    document_id: document.id,
    operation: 'chunk_and_embed',
    user_id: userId,
    content, // Store content in job payload for processing
    processing_options: {
      chunking: CHUNKING_PRESETS.balanced
    }
  }
  
  const jobStartTime = performance.now()
  const { data: job, error: jobError } = await supabase
    .from('rag_ingest_jobs')
    .insert({
      owner: userId,
      payload: jobPayload,
      status: 'queued'
    })
    .select('*')
    .single()
  
  trackDatabaseQuery('rag_jobs_insert', jobStartTime)
  
  if (jobError || !job) {
    throw new Error(`Failed to create processing job: ${jobError?.message}`)
  }
  
  // In a real production environment, you would trigger background processing here
  // For now, we'll return the job and it can be processed via API call
  
  return { document: document as RAGDocument, job: job as RAGIngestJob }
}

// Get processing statistics for monitoring
export async function getProcessingStats(userId: string): Promise<ProcessingStats> {
  const supabase = await createClient()
  
  // Get user's jobs
  const { data: jobs, error: jobsError } = await supabase
    .from('rag_ingest_jobs')
    .select('*')
    .eq('owner', userId)
    .order('created_at', { ascending: false })
    .limit(1000) // Reasonable limit for stats
  
  if (jobsError) {
    throw new Error(`Failed to fetch processing stats: ${jobsError.message}`)
  }
  
  // Get chunk count for user
  const { count: chunkCount, error: chunkError } = await supabase
    .from('rag_chunks')
    .select('*', { count: 'exact', head: true })
    .in('doc_id', []) // We'd need to join with documents to filter by user
  
  const completedJobs = (jobs || []).filter(job => job.status === 'completed')
  const errorJobs = (jobs || []).filter(job => job.status === 'error')
  
  const totalProcessingTime = completedJobs.reduce((sum, job) => {
    const results = (job.payload as Record<string, unknown>)?.results as Record<string, unknown> | undefined
    return sum + (results?.processingTimeMs || 0)
  }, 0)
  
  const totalTokens = completedJobs.reduce((sum, job) => {
    const results = (job.payload as Record<string, unknown>)?.results as Record<string, unknown> | undefined
    return sum + (results?.totalTokens || 0)
  }, 0)
  
  const totalChunks = completedJobs.reduce((sum, job) => {
    const results = (job.payload as Record<string, unknown>)?.results as Record<string, unknown> | undefined
    return sum + (results?.chunksCreated || 0)
  }, 0)
  
  return {
    documentsProcessed: completedJobs.length,
    chunksCreated: totalChunks,
    totalTokens,
    averageProcessingTime: completedJobs.length > 0 ? totalProcessingTime / completedJobs.length : 0,
    errors: errorJobs.map(job => job.error || 'Unknown error')
  }
}